"""í´ëŸ¬ìŠ¤í„°ë§ API ì—”ë“œí¬ì¸íŠ¸"""
import sys
import traceback

try:
    from fastapi import APIRouter, HTTPException, Depends
except Exception as e:
    traceback.print_exc(file=sys.stderr)
    raise

try:
    from pydantic import BaseModel
except Exception as e:
    traceback.print_exc(file=sys.stderr)
    raise

from typing import List, Optional, Dict, Any
from sqlalchemy.ext.asyncio import AsyncSession
import pandas as pd
import numpy as np
import uuid
import json
import logging
from pathlib import Path
from collections import Counter
import os
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import NearestNeighbors
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances

try:
    from app.db.session import get_session
except Exception as e:
    traceback.print_exc(file=sys.stderr)
    raise

try:
    from app.db.dao_panels import extract_features_for_clustering
except Exception as e:
    traceback.print_exc(file=sys.stderr)
    raise

try:
    from app.clustering.integrated_pipeline import IntegratedClusteringPipeline
except Exception as e:
    traceback.print_exc(file=sys.stderr)
    raise

try:
    from app.clustering.data_preprocessor import preprocess_for_clustering
except Exception as e:
    traceback.print_exc(file=sys.stderr)
    raise

try:
    from app.clustering.filters.panel_filter import PanelFilter
except Exception as e:
    traceback.print_exc(file=sys.stderr)
    raise

try:
    from app.clustering.processors.vector_processor import VectorProcessor
except Exception as e:
    traceback.print_exc(file=sys.stderr)
    raise

try:
    from app.clustering.algorithms.kmeans import KMeansAlgorithm
except Exception as e:
    traceback.print_exc(file=sys.stderr)
    raise

try:
    from app.clustering.algorithms.minibatch_kmeans import MiniBatchKMeansAlgorithm
except Exception as e:
    traceback.print_exc(file=sys.stderr)
    raise

try:
    from app.clustering.algorithms.hdbscan import HDBSCANAlgorithm
except Exception as e:
    traceback.print_exc(file=sys.stderr)
    raise

try:
    from app.clustering.artifacts import save_artifacts, new_session_dir
except Exception as e:
    traceback.print_exc(file=sys.stderr)
    raise

try:
    router = APIRouter(prefix="/api/clustering", tags=["clustering"])
except Exception as e:
    traceback.print_exc(file=sys.stderr)
    raise


class ClusterRequest(BaseModel):
    """í´ëŸ¬ìŠ¤í„°ë§ ìš”ì²­"""
    panel_ids: List[str]
    algo: str = "auto"  # "auto", "kmeans", "minibatch_kmeans", "hdbscan"
    n_clusters: Optional[int] = None  # Noneì´ë©´ ìë™ ì„ íƒ
    use_dynamic_strategy: bool = True  # ë™ì  ì „ëµ ì‚¬ìš© ì—¬ë¶€
    filter_params: Optional[Dict[str, Any]] = None
    processor_params: Optional[Dict[str, Any]] = None
    algorithm_params: Optional[Dict[str, Any]] = None
    sample_size: Optional[int] = None  # ìƒ˜í”Œë§ í¬ê¸° (Noneì´ë©´ ì „ì²´ ë°ì´í„° ì‚¬ìš©)


class CompareRequest(BaseModel):
    """ê·¸ë£¹ ë¹„êµ ìš”ì²­"""
    session_id: str
    c1: int
    c2: int


class UMAPRequest(BaseModel):
    """UMAP 2D ì¢Œí‘œ ìš”ì²­"""
    session_id: str
    sample: Optional[int] = None
    metric: str = "cosine"
    n_neighbors: int = 15
    min_dist: float = 0.1
    seed: Optional[int] = 0


@router.post("/cluster-from-csv")
async def cluster_from_csv(
    req: ClusterRequest
):
    """
    CSV íŒŒì¼ì—ì„œ ì§ì ‘ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰ (DB ì—°ë™ ì—†ì´)
    """
    logger = logging.getLogger(__name__)
    
    debug_info = {
        'step': 'start',
        'errors': []
    }
    
    try:
        logger.info("[CSV í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘]")
        debug_info['step'] = 'load_csv'
        
        # CSV íŒŒì¼ì€ ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ - NeonDBì—ì„œë§Œ ë°ì´í„° ë¡œë“œ
        error_msg = "CSV íŒŒì¼ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ì€ ë” ì´ìƒ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. NeonDBì— ë°ì´í„°ë¥¼ ë§ˆì´ê·¸ë ˆì´ì…˜í•˜ì„¸ìš”."
        debug_info['errors'].append(error_msg)
        logger.error(f"[CSV íŒŒì¼ ì˜¤ë¥˜] {error_msg}")
        raise HTTPException(
            status_code=400,
            detail=json.dumps({
                "error": error_msg,
                "debug": debug_info
            }, ensure_ascii=False)
        )
        
        # CSV íŒŒì¼ ë¡œë“œ
        import pandas as pd
        try:
            logger.info(f"[CSV íŒŒì¼ ë¡œë“œ ì‹œë„] ê²½ë¡œ: {csv_path}, í¬ê¸°: {csv_path.stat().st_size if csv_path.exists() else 'N/A'} bytes")
            df_raw = pd.read_csv(csv_path, encoding='utf-8')
            logger.info(f"[CSV ë¡œë“œ ì„±ê³µ] {len(df_raw)}í–‰, {len(df_raw.columns)}ì—´")
            debug_info['csv_load_success'] = True
            debug_info['csv_rows'] = len(df_raw)
            debug_info['csv_columns'] = len(df_raw.columns)
        except Exception as csv_load_error:
            error_msg = f"CSV íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {str(csv_load_error)}"
            debug_info['errors'].append(error_msg)
            logger.error(f"[CSV ë¡œë“œ ì˜¤ë¥˜] {error_msg}", exc_info=True)
            raise HTTPException(
                status_code=400,
                detail=json.dumps({
                    "error": error_msg,
                    "debug": debug_info
                }, ensure_ascii=False)
            )
        
        # ìƒ˜í”Œë§ ì˜µì…˜ (ê¸°ë³¸ê°’: ì „ì²´ ë°ì´í„° ì‚¬ìš©, sample_sizeê°€ ì§€ì •ë˜ë©´ ìƒ˜í”Œë§)
        import time
        import random
        sample_size = req.sample_size if hasattr(req, 'sample_size') and req.sample_size else None
        
        if sample_size and len(df_raw) > sample_size:
            # ë§¤ë²ˆ ë‹¤ë¥¸ ëœë¤ ìƒ˜í”Œì„ ìœ„í•´ ì‹œê°„ ê¸°ë°˜ ì‹œë“œ ì‚¬ìš©
            random_seed = int(time.time() * 1000) % (2**31)  # ë°€ë¦¬ì´ˆ ê¸°ë°˜ ì‹œë“œ
            df_raw = df_raw.sample(n=sample_size, random_state=random_seed).reset_index(drop=True)
            logger.info(f"[ìƒ˜í”Œë§] {sample_size}ê°œ í–‰ ë¬´ì‘ìœ„ ì„ íƒ (ì‹œë“œ: {random_seed})")
            debug_info['sample_seed'] = random_seed
            debug_info['sample_size'] = sample_size
        else:
            logger.info(f"[ì „ì²´ ë°ì´í„° ì‚¬ìš©] {len(df_raw)}ê°œ í–‰ ëª¨ë‘ ì‚¬ìš©")
            debug_info['sample_size'] = None
            debug_info['use_full_data'] = True
        
        # mb_sn ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ìƒì„±
        if 'mb_sn' not in df_raw.columns:
            df_raw['mb_sn'] = [f"csv_panel_{i}" for i in range(len(df_raw))]
        
        debug_info['step'] = 'preprocess'
        debug_info['raw_data_count'] = len(df_raw)
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        try:
            logger.info(f"[ì „ì²˜ë¦¬ ì‹œì‘] ì›ë³¸ ë°ì´í„°: {len(df_raw)}í–‰")
            # CSV ë°ì´í„°ë¥¼ dict ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (preprocess_for_clustering í˜¸í™˜ í˜•ì‹)
            panel_data = df_raw.to_dict('records')
            logger.info(f"[ì „ì²˜ë¦¬] dict ë³€í™˜ ì™„ë£Œ: {len(panel_data)}ê°œ ë ˆì½”ë“œ")
            
            df = preprocess_for_clustering(panel_data, verbose=False)
            logger.info(f"[ì „ì²˜ë¦¬ ì™„ë£Œ] ì „ì²˜ë¦¬ëœ ë°ì´í„° í–‰ ìˆ˜: {len(df)}, ì—´ ìˆ˜: {len(df.columns) if len(df) > 0 else 0}")
            if len(df) > 0:
                logger.info(f"[ì „ì²˜ë¦¬ ì™„ë£Œ] ì»¬ëŸ¼ ëª©ë¡ (ì²˜ìŒ 10ê°œ): {list(df.columns)[:10]}")
            debug_info['preprocessed_data_count'] = len(df)
            debug_info['preprocessed_columns'] = list(df.columns) if len(df) > 0 else []
            debug_info['preprocess_success'] = True
        except Exception as preprocess_error:
            import traceback
            error_trace = traceback.format_exc()
            error_msg = f'ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(preprocess_error)}'
            debug_info['errors'].append(error_msg)
            debug_info['preprocess_error_trace'] = error_trace
            logger.error(f"[ì „ì²˜ë¦¬ ì˜¤ë¥˜] {error_msg}\n{error_trace}", exc_info=True)
            raise HTTPException(
                status_code=400,
                detail=json.dumps({
                    "error": error_msg,
                    "debug": debug_info
                }, ensure_ascii=False)
            )
        
        if len(df) == 0:
            debug_info['errors'].append('ì „ì²˜ë¦¬ í›„ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.')
            raise HTTPException(
                status_code=400,
                detail=json.dumps({
                    "error": "ì „ì²˜ë¦¬ í›„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "debug": debug_info
                }, ensure_ascii=False)
            )
        
        debug_info['step'] = 'check_sample_size'
        debug_info['sample_size'] = len(df)
        
        # ìƒ˜í”Œ ìˆ˜ í™•ì¸ ë° ê²½ê³ 
        if len(df) < 100:
            debug_info['warnings'] = [f'ìƒ˜í”Œ ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ ({len(df)}ê°œ < 100ê°œ). ë™ì  ì „ëµì— ë”°ë¼ í”„ë¡œíŒŒì¼ë§ë§Œ ì œê³µë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.']
            logger.warning(f"[ìƒ˜í”Œ ìˆ˜ ë¶€ì¡±] {len(df)}ê°œ íŒ¨ë„ - í”„ë¡œíŒŒì¼ë§ë§Œ ì œê³µë  ìˆ˜ ìˆìŒ")
        
        # ë‚˜ë¨¸ì§€ ë¡œì§ì€ ê¸°ì¡´ cluster_panelsì™€ ë™ì¼
        return await _execute_clustering(
            df=df,
            req=req,
            debug_info=debug_info,
            logger=logger
        )
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        debug_info['step'] = 'error'
        error_msg = f'ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}'
        debug_info['errors'].append(error_msg)
        debug_info['error_trace'] = error_trace
        debug_info['error_type'] = type(e).__name__
        logger.error(f"[CSV í´ëŸ¬ìŠ¤í„°ë§ ì˜¤ë¥˜] {error_msg}\n{error_trace}", exc_info=True)
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=json.dumps({
                "error": error_msg,
                "error_type": type(e).__name__,
                "debug": debug_info
            }, ensure_ascii=False)
        )


async def _execute_clustering(
    df: pd.DataFrame,
    req: ClusterRequest,
    debug_info: Dict[str, Any],
    logger: logging.Logger
):
    """ê³µí†µ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰ ë¡œì§"""
    # 3. ì•Œê³ ë¦¬ì¦˜ ì„ íƒ
    algorithm = None
    if req.algo != "auto":
        if req.algo == "kmeans":
            algorithm = KMeansAlgorithm(
                n_clusters=req.n_clusters or 8
            )
        elif req.algo == "minibatch_kmeans":
            algorithm = MiniBatchKMeansAlgorithm(
                n_clusters=req.n_clusters or 8
            )
        elif req.algo == "hdbscan":
            algorithm = HDBSCANAlgorithm()
    
    # 4. íŒŒì´í”„ë¼ì¸ êµ¬ì„±
    pipeline = IntegratedClusteringPipeline(
        filter=PanelFilter(),
        processor=VectorProcessor(),
        algorithm=algorithm,
        use_dynamic_strategy=req.use_dynamic_strategy
    )
    
    debug_info['step'] = 'clustering'
    
    # 5. í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰
    try:
        # mb_sn ì»¬ëŸ¼ ì œì™¸í•˜ê³  í´ëŸ¬ìŠ¤í„°ë§
        df_for_clustering = df.drop(columns=['mb_sn']) if 'mb_sn' in df.columns else df
        result = pipeline.cluster(df_for_clustering, verbose=False)
        logger.info(f"[í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ] ì„±ê³µ: {result.get('success', False)}")
    except Exception as clustering_error:
        debug_info['errors'].append(f'í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰ ì‹¤íŒ¨: {str(clustering_error)}')
        logger.error(f"[í´ëŸ¬ìŠ¤í„°ë§ ì˜¤ë¥˜] {str(clustering_error)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=json.dumps({
                "error": f"í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰ ì‹¤íŒ¨: {str(clustering_error)}",
                "debug": debug_info
            }, ensure_ascii=False)
        )
    
    debug_info['step'] = 'complete'
    
    if not result.get('success'):
        # í”„ë¡œíŒŒì¼ë§ ëª¨ë“œ
        return {
            "success": False,
            "session_id": None,
            "n_samples": result.get('n_samples', 0),
            "n_clusters": 0,
            "labels": [],
            "cluster_sizes": {},
            "meta": {
                "filter_info": result.get('filter_info'),
                "processor_info": result.get('processor_info'),
                "algorithm_info": result.get('algorithm_info'),
            },
            "profile": result.get('profile'),
            "reason": result.get('reason', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'),
            "debug": {
                **debug_info,
                "sample_size": result.get('n_samples', 0),
                "warnings": debug_info.get('warnings', []) + [result.get('reason', '')]
            }
        }
    
    # 6. ì„¸ì…˜ ìƒì„± ë° ì•„í‹°íŒ©íŠ¸ ì €ì¥
    session_dir = new_session_dir()
    session_id = session_dir.name  # ë””ë ‰í† ë¦¬ ì´ë¦„ì´ session_id
    
    # labels ì¶”ì¶œ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
    labels = None
    if result.get('labels') is not None:
        # resultì— labelsê°€ ì§ì ‘ ìˆëŠ” ê²½ìš°
        labels = result['labels']
        if hasattr(labels, 'tolist'):
            labels = labels.tolist()
        elif isinstance(labels, np.ndarray):
            labels = labels.tolist()
        elif not isinstance(labels, list):
            labels = list(labels)
    elif 'data' in result and isinstance(result['data'], pd.DataFrame):
        # DataFrameì— cluster ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°
        if 'cluster' in result['data'].columns:
            labels = result['data']['cluster'].tolist()
    
    if labels is None:
        labels = []
        logger.warning("[ì„¸ì…˜ ì €ì¥] labelsë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # cluster_sizes ê³„ì‚°
    cluster_sizes = result.get('cluster_sizes', {})
    if not cluster_sizes and labels:
        # labelsì—ì„œ cluster_sizes ê³„ì‚°
        from collections import Counter
        label_counts = Counter(labels)
        cluster_sizes = {k: int(v) for k, v in label_counts.items() if k != -1}  # ë…¸ì´ì¦ˆ ì œì™¸, í‚¤ë¥¼ ì •ìˆ˜ë¡œ ìœ ì§€
    
    # n_clusters ê³„ì‚° (cluster_sizes ë˜ëŠ” labelsì—ì„œ)
    n_clusters = result.get('n_clusters', 0)
    if n_clusters == 0:
        if cluster_sizes:
            # cluster_sizesì—ì„œ ë…¸ì´ì¦ˆ(-1) ì œì™¸í•œ í´ëŸ¬ìŠ¤í„° ìˆ˜ ê³„ì‚°
            valid_clusters = [k for k in cluster_sizes.keys() if k != -1 and k != '-1' and k != '-1.0']
            n_clusters = len(valid_clusters)
            logger.info(f"[í´ëŸ¬ìŠ¤í„° ìˆ˜ ê³„ì‚°] cluster_sizesì—ì„œ ê³„ì‚°: {n_clusters}ê°œ (keys: {list(cluster_sizes.keys())[:10]})")
        elif labels:
            # labelsì—ì„œ ê³ ìœ  í´ëŸ¬ìŠ¤í„° ìˆ˜ ê³„ì‚°
            unique_labels = set(labels)
            unique_labels.discard(-1)  # ë…¸ì´ì¦ˆ ì œì™¸
            n_clusters = len(unique_labels)
            logger.info(f"[í´ëŸ¬ìŠ¤í„° ìˆ˜ ê³„ì‚°] labelsì—ì„œ ê³„ì‚°: {n_clusters}ê°œ (unique: {sorted(unique_labels)[:10]})")
    
    logger.info(f"[í´ëŸ¬ìŠ¤í„° ìˆ˜ ìµœì¢…] n_clusters={n_clusters}, cluster_sizes_keys={list(cluster_sizes.keys())[:10] if cluster_sizes else []}, labels_unique={len(set(labels)) if labels else 0}")
    
    # ì•„í‹°íŒ©íŠ¸ ì €ì¥
    result_data = result.get('data', df)
    
    # labelsë¥¼ numpy arrayë¡œ ë³€í™˜ (save_artifactsê°€ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹)
    labels_array = np.array(labels) if labels else None
    
    # í”¼ì²˜ íƒ€ì… ì •ë³´ ì¶”ì¶œ
    from app.clustering.data_preprocessor import get_feature_types
    feature_types = get_feature_types(result_data)
    
    logger.info(f"[í”¼ì²˜ íƒ€ì… ì¶”ì¶œ] bin: {len(feature_types.get('bin_cols', []))}, cat: {len(feature_types.get('cat_cols', []))}, num: {len(feature_types.get('num_cols', []))}")
    
    save_artifacts(
        session_dir,
        result_data,
        labels_array,
        {
            'request': req.dict(),
            'feature_types': feature_types,  # í”¼ì²˜ íƒ€ì… ì •ë³´ ì¶”ê°€
            'result_meta': {
                'success': result.get('success', False),
                'n_clusters': result.get('n_clusters'),
                'optimal_k': result.get('optimal_k'),
                'k_scores': result.get('k_scores', []),
                'strategy': result.get('strategy'),
                'filter_info': result.get('filter_info'),
                'processor_info': result.get('processor_info'),
                'algorithm_info': {
                    **(result.get('algorithm_info', {}) or {}),
                    'features': result.get('features', []),
                }
            }
        }
    )
    
    logger.info(f"[ì„¸ì…˜ ìƒì„±] {session_id}")
    
    # ë©”íŠ¸ë¦­ ì¶”ì¶œ (ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„)
    silhouette_score = result.get('silhouette_score') or result.get('algorithm_info', {}).get('silhouette_score')
    davies_bouldin_score = result.get('davies_bouldin_score') or result.get('algorithm_info', {}).get('davies_bouldin_score')
    calinski_harabasz = result.get('calinski_harabasz_score') or result.get('algorithm_info', {}).get('calinski_harabasz')
    
    logger.info(f"[ë©”íŠ¸ë¦­ ì¶”ì¶œ] silhouette={silhouette_score}, davies_bouldin={davies_bouldin_score}, calinski={calinski_harabasz}")
    
    return {
        "success": True,
        "session_id": session_id,
        "n_samples": result.get('n_samples', len(df)),
        "n_clusters": n_clusters,  # ê³„ì‚°ëœ n_clusters ì‚¬ìš©
        "labels": labels,
        "cluster_sizes": cluster_sizes,
        "silhouette_score": silhouette_score,
        "davies_bouldin_score": davies_bouldin_score,
        "calinski_harabasz_score": calinski_harabasz,
        "optimal_k": result.get('optimal_k'),
        "strategy": result.get('strategy'),
        "meta": {
            "filter_info": result.get('filter_info'),
            "processor_info": result.get('processor_info'),
            "algorithm_info": {
                **(result.get('algorithm_info', {}) or {}),
                "features": result.get('features', []),
                "silhouette_score": silhouette_score,
                "davies_bouldin_score": davies_bouldin_score,
                "calinski_harabasz": calinski_harabasz,
            }
        },
        "debug": debug_info
    }


@router.post("/cluster")
async def cluster_panels(
    req: ClusterRequest,
    session: AsyncSession = Depends(get_session)
):
    """
    í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰ (DBì—ì„œ ë°ì´í„° ì¶”ì¶œ)
    """
    logger = logging.getLogger(__name__)
    
    debug_info = {
        'step': 'start',
        'panel_ids_count': len(req.panel_ids),
        'errors': []
    }
    
    try:
        logger.info(f"[í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘] íŒ¨ë„ ìˆ˜: {len(req.panel_ids)}")
        debug_info['step'] = 'extract_data'
        
        # 1. íŒ¨ë„ ë°ì´í„° ì¶”ì¶œ
        panel_data = await extract_features_for_clustering(session, req.panel_ids)
        logger.info(f"[ë°ì´í„° ì¶”ì¶œ] ì¶”ì¶œëœ íŒ¨ë„ ìˆ˜: {len(panel_data) if panel_data else 0}")
        
        if not panel_data:
            debug_info['errors'].append('íŒ¨ë„ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: DBì—ì„œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
            raise HTTPException(
                status_code=404,
                detail=json.dumps({
                    "error": "íŒ¨ë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "debug": debug_info
                }, ensure_ascii=False)
            )
        
        debug_info['step'] = 'preprocess'
        debug_info['raw_data_count'] = len(panel_data)
        
        # 2. ë°ì´í„° ì „ì²˜ë¦¬ (ì›ì‹œ ë°ì´í„° -> í´ëŸ¬ìŠ¤í„°ë§ìš© DataFrame)
        try:
            df = preprocess_for_clustering(panel_data, verbose=False)
            logger.info(f"[ì „ì²˜ë¦¬ ì™„ë£Œ] ì „ì²˜ë¦¬ëœ ë°ì´í„° í–‰ ìˆ˜: {len(df)}, ì—´ ìˆ˜: {len(df.columns) if len(df) > 0 else 0}")
            debug_info['preprocessed_data_count'] = len(df)
            debug_info['preprocessed_columns'] = list(df.columns) if len(df) > 0 else []
        except Exception as preprocess_error:
            debug_info['errors'].append(f'ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(preprocess_error)}')
            logger.error(f"[ì „ì²˜ë¦¬ ì˜¤ë¥˜] {str(preprocess_error)}", exc_info=True)
            raise HTTPException(
                status_code=400,
                detail=json.dumps({
                    "error": f"ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(preprocess_error)}",
                    "debug": debug_info
                }, ensure_ascii=False)
            )
        
        if len(df) == 0:
            debug_info['errors'].append('ì „ì²˜ë¦¬ í›„ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.')
            raise HTTPException(
                status_code=400,
                detail=json.dumps({
                    "error": "ì „ì²˜ë¦¬ í›„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "debug": debug_info
                }, ensure_ascii=False)
            )
        
        debug_info['step'] = 'check_sample_size'
        debug_info['sample_size'] = len(df)
        
        # ìƒ˜í”Œ ìˆ˜ í™•ì¸ ë° ê²½ê³ 
        if len(df) < 100:
            debug_info['warnings'] = [f'ìƒ˜í”Œ ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ ({len(df)}ê°œ < 100ê°œ). ë™ì  ì „ëµì— ë”°ë¼ í”„ë¡œíŒŒì¼ë§ë§Œ ì œê³µë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.']
            logger.warning(f"[ìƒ˜í”Œ ìˆ˜ ë¶€ì¡±] {len(df)}ê°œ íŒ¨ë„ - í”„ë¡œíŒŒì¼ë§ë§Œ ì œê³µë  ìˆ˜ ìˆìŒ")
        
        # ë‚˜ë¨¸ì§€ ë¡œì§ì€ ê³µí†µ í•¨ìˆ˜ë¡œ ë¶„ë¦¬
        return await _execute_clustering(
            df=df,
            req=req,
            debug_info=debug_info,
            logger=logger
        )
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        debug_info['step'] = 'error'
        debug_info['errors'].append(f'ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}')
        logger.error(f"[í´ëŸ¬ìŠ¤í„°ë§ ì˜¤ë¥˜] {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=json.dumps({
                "error": f"í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {str(e)}",
                "debug": debug_info
            }, ensure_ascii=False)
    )


@router.post("/compare")
async def compare_clusters(req: CompareRequest):
    """
    êµ°ì§‘ ë¹„êµ ë¶„ì„
    """
    logger = logging.getLogger(__name__)
    
    try:
        logger.info(f"[ë¹„êµ ë¶„ì„ ì‹œì‘] session_id: {req.session_id}, c1: {req.c1}, c2: {req.c2}")
        
        from app.clustering.compare import compare_groups
        
        # ì„¸ì…˜ì—ì„œ ë°ì´í„° ë¡œë“œ
        from app.clustering.artifacts import load_artifacts
        logger.info(f"[ë¹„êµ ë¶„ì„] ì•„í‹°íŒ©íŠ¸ ë¡œë“œ ì‹œë„: {req.session_id}")
        artifacts = load_artifacts(req.session_id)
        
        if artifacts is None:
            logger.error(f"[ë¹„êµ ë¶„ì„ ì˜¤ë¥˜] ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {req.session_id}")
            raise HTTPException(
                status_code=404,
                detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
        
        logger.info(f"[ë¹„êµ ë¶„ì„] ì•„í‹°íŒ©íŠ¸ ë¡œë“œ ì„±ê³µ, keys: {list(artifacts.keys())}")
        
        df = artifacts.get('data')
        labels = artifacts.get('labels')
        
        logger.info(f"[ë¹„êµ ë¶„ì„] ë°ì´í„° í™•ì¸] df: {df is not None}, labels: {labels is not None}")
        if df is not None:
            logger.info(f"[ë¹„êµ ë¶„ì„] DataFrame ì •ë³´] í–‰ ìˆ˜: {len(df)}, ì—´ ìˆ˜: {len(df.columns)}, ì»¬ëŸ¼: {list(df.columns)[:10]}")
        if labels is not None:
            logger.info(f"[ë¹„êµ ë¶„ì„] Labels ì •ë³´] íƒ€ì…: {type(labels)}, ê¸¸ì´: {len(labels) if hasattr(labels, '__len__') else 'N/A'}, ê³ ìœ ê°’: {sorted(set(labels))[:10] if hasattr(labels, '__iter__') else 'N/A'}")
        
        if df is None or labels is None:
            logger.error(f"[ë¹„êµ ë¶„ì„ ì˜¤ë¥˜] ë°ì´í„° ì—†ìŒ] df: {df is None}, labels: {labels is None}")
            raise HTTPException(
                status_code=400,
                detail="í´ëŸ¬ìŠ¤í„°ë§ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
            )
        
        # DataFrameì´ ë¬¸ìì—´ ê²½ë¡œì¸ ê²½ìš° ë¡œë“œ
        if isinstance(df, str):
            logger.info(f"[ë¹„êµ ë¶„ì„] DataFrame ê²½ë¡œì—ì„œ ë¡œë“œ: {df}")
            import pandas as pd
            df = pd.read_csv(df)
            logger.info(f"[ë¹„êµ ë¶„ì„] DataFrame ë¡œë“œ ì™„ë£Œ] í–‰ ìˆ˜: {len(df)}, ì—´ ìˆ˜: {len(df.columns)}")
        
        # labelsë¥¼ numpy arrayë¡œ ë³€í™˜
        if not isinstance(labels, np.ndarray):
            if isinstance(labels, list):
                labels = np.array(labels)
                logger.info(f"[ë¹„êµ ë¶„ì„] Labelsë¥¼ numpy arrayë¡œ ë³€í™˜] ê¸¸ì´: {len(labels)}")
            else:
                logger.error(f"[ë¹„êµ ë¶„ì„ ì˜¤ë¥˜] Labels íƒ€ì… ì˜¤ë¥˜] íƒ€ì…: {type(labels)}")
                raise HTTPException(
                    status_code=400,
                    detail=f"Labels íƒ€ì… ì˜¤ë¥˜: {type(labels)}"
                )
        
        # í´ëŸ¬ìŠ¤í„° ID í™•ì¸
        unique_labels = sorted(set(labels))
        logger.info(f"[ë¹„êµ ë¶„ì„] ê³ ìœ  í´ëŸ¬ìŠ¤í„° ID] {unique_labels[:20]}")
        if req.c1 not in unique_labels:
            logger.warning(f"[ë¹„êµ ë¶„ì„ ê²½ê³ ] í´ëŸ¬ìŠ¤í„° {req.c1}ê°€ labelsì— ì—†ìŒ. ì‚¬ìš© ê°€ëŠ¥í•œ ID: {unique_labels[:10]}")
        if req.c2 not in unique_labels:
            logger.warning(f"[ë¹„êµ ë¶„ì„ ê²½ê³ ] í´ëŸ¬ìŠ¤í„° {req.c2}ê°€ labelsì— ì—†ìŒ. ì‚¬ìš© ê°€ëŠ¥í•œ ID: {unique_labels[:10]}")
        
        # ë¹„êµ ë¶„ì„ ì‹¤í–‰
        # ë©”íƒ€ë°ì´í„°ì—ì„œ í”¼ì²˜ íƒ€ì… ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        feature_types = artifacts.get('meta', {}).get('feature_types', {})
        bin_cols = feature_types.get('bin_cols', [])
        cat_cols = feature_types.get('cat_cols', [])
        num_cols = feature_types.get('num_cols', [])
        
        logger.info(f"[ë¹„êµ ë¶„ì„] í”¼ì²˜ íƒ€ì… ì •ë³´] bin_cols: {len(bin_cols)}, cat_cols: {len(cat_cols)}, num_cols: {len(num_cols)}")
        
        # í”¼ì²˜ íƒ€ì… ì •ë³´ê°€ ì—†ìœ¼ë©´ ìë™ ê°ì§€
        if not bin_cols and not cat_cols and not num_cols:
            logger.info("[ë¹„êµ ë¶„ì„] í”¼ì²˜ íƒ€ì… ìë™ ê°ì§€ ì‹œë„")
            try:
                from app.clustering.data_preprocessor import get_feature_types
                feature_types = get_feature_types(df)
                bin_cols = feature_types.get('bin_cols', [])
                cat_cols = feature_types.get('cat_cols', [])
                num_cols = feature_types.get('num_cols', [])
                logger.info(f"[ë¹„êµ ë¶„ì„] ìë™ ê°ì§€ ì™„ë£Œ] bin_cols: {len(bin_cols)}, cat_cols: {len(cat_cols)}, num_cols: {len(num_cols)}")
            except Exception as e:
                logger.warning(f"[ë¹„êµ ë¶„ì„] í”¼ì²˜ íƒ€ì… ìë™ ê°ì§€ ì‹¤íŒ¨: {str(e)}, ê¸°ë³¸ê°’ ì‚¬ìš©")
                bin_cols = []
                cat_cols = []
                num_cols = []
        
        logger.info(f"[ë¹„êµ ë¶„ì„] compare_groups í˜¸ì¶œ] c1: {req.c1}, c2: {req.c2}")
        comparison = compare_groups(
            df,
            labels,
            req.c1,
            req.c2,
            bin_cols=bin_cols,
            cat_cols=cat_cols,
            num_cols=num_cols
        )
        
        logger.info(f"[ë¹„êµ ë¶„ì„ ì™„ë£Œ] comparison keys: {list(comparison.keys())}, comparison count: {len(comparison.get('comparison', []))}")
        
        return comparison
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        logger.error(f"[ë¹„êµ ë¶„ì„ ì˜¤ë¥˜] {str(e)}\n{error_trace}")
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"ë¹„êµ ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
    )


@router.post("/umap")
async def get_umap_coordinates(req: UMAPRequest):
    """
    UMAP 2D ì¢Œí‘œ ê³„ì‚°
    """
    logger = logging.getLogger(__name__)
    
    try:
        from umap import UMAP
        
        logger.info(f"[UMAP ì‹œì‘] session_id: {req.session_id}, sample: {req.sample}")
        
        # ì„¸ì…˜ì—ì„œ ë°ì´í„° ë¡œë“œ
        from app.clustering.artifacts import load_artifacts
        artifacts = load_artifacts(req.session_id)
        
        if artifacts is None:
            logger.error(f"[UMAP ì˜¤ë¥˜] ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {req.session_id}")
            raise HTTPException(
                status_code=404,
                detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
        
        df = artifacts.get('data')
        labels_raw = artifacts.get('labels')
        
        logger.info(f"[UMAP ë°ì´í„° ë¡œë“œ] df: {df is not None}, labels: {labels_raw is not None}")
        if df is not None:
            logger.info(f"[UMAP ë°ì´í„° ì •ë³´] í–‰ ìˆ˜: {len(df)}, ì—´ ìˆ˜: {len(df.columns)}")
        if labels_raw is not None:
            logger.info(f"[UMAP ë¼ë²¨ ì •ë³´] íƒ€ì…: {type(labels_raw)}, ê¸¸ì´: {len(labels_raw) if hasattr(labels_raw, '__len__') else 'N/A'}")
        
        if df is None:
            logger.error("[UMAP ì˜¤ë¥˜] ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            raise HTTPException(
                status_code=400,
                detail="ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
            )

        # labels ì²˜ë¦¬ (numpy arrayì¸ ê²½ìš° ì²˜ë¦¬)
        labels = None
        if labels_raw is not None:
            if isinstance(labels_raw, np.ndarray):
                labels = labels_raw.tolist()
                logger.info(f"[UMAP ë¼ë²¨ ë³€í™˜] numpy array -> list, ê¸¸ì´: {len(labels)}")
            elif isinstance(labels_raw, list):
                labels = labels_raw
                logger.info(f"[UMAP ë¼ë²¨] ì´ë¯¸ list í˜•ì‹, ê¸¸ì´: {len(labels)}")
            else:
                # ë‹¤ë¥¸ íƒ€ì…ì¸ ê²½ìš° ë³€í™˜ ì‹œë„
                try:
                    labels = list(labels_raw)
                    logger.info(f"[UMAP ë¼ë²¨ ë³€í™˜] ë‹¤ë¥¸ íƒ€ì… -> list, ê¸¸ì´: {len(labels)}")
                except Exception as e:
                    logger.warning(f"[UMAP ë¼ë²¨ ë³€í™˜ ì‹¤íŒ¨] {str(e)}")
                    labels = []
        else:
            logger.warning("[UMAP ë¼ë²¨] labelsê°€ Noneì…ë‹ˆë‹¤.")
            labels = []

        # UMAP ì ìš©
        umap = UMAP(
            n_components=2,
            n_neighbors=req.n_neighbors,
            min_dist=req.min_dist,
            metric=req.metric,
            random_state=req.seed
        )
        
        # ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
        logger.info(f"[UMAP í”¼ì³ ì„ íƒ] ìˆ«ìí˜• ì»¬ëŸ¼: {len(numeric_cols)}ê°œ")
        X = df[numeric_cols].fillna(0).values
        
        # ìƒ˜í”Œë§
        sample_indices = None
        if req.sample and req.sample < len(X):
            np.random.seed(req.seed)
            sample_indices = np.random.choice(len(X), req.sample, replace=False)
            X = X[sample_indices]
            df_sample = df.iloc[sample_indices]
            logger.info(f"[UMAP ìƒ˜í”Œë§] {len(X)}ê°œ ìƒ˜í”Œ ì„ íƒ (ì „ì²´: {len(df)}ê°œ)")
        else:
            df_sample = df
            logger.info(f"[UMAP ìƒ˜í”Œë§] ì „ì²´ ë°ì´í„° ì‚¬ìš©: {len(X)}ê°œ")
        
        # UMAP ë³€í™˜
        logger.info("[UMAP ë³€í™˜ ì‹œì‘]")
        coords = umap.fit_transform(X)
        logger.info(f"[UMAP ë³€í™˜ ì™„ë£Œ] ì¢Œí‘œ ìˆ˜: {len(coords)}")
        
        # labels ìƒ˜í”Œë§ (ìƒ˜í”Œë§ëœ ê²½ìš°)
        if sample_indices is not None and labels:
            sampled_labels = [labels[i] for i in sample_indices]
            logger.info(f"[UMAP ë¼ë²¨ ìƒ˜í”Œë§] {len(sampled_labels)}ê°œ ë¼ë²¨ ì„ íƒ")
        else:
            sampled_labels = labels[:len(coords)] if labels else []
            logger.info(f"[UMAP ë¼ë²¨] ì „ì²´ ë¼ë²¨ ì‚¬ìš©: {len(sampled_labels)}ê°œ")
        
        # panel_ids ì¶”ì¶œ
        if 'mb_sn' in df_sample.columns:
            panel_ids = df_sample['mb_sn'].tolist()
        else:
            panel_ids = df_sample.index.astype(str).tolist()
        
        logger.info(f"[UMAP ì™„ë£Œ] ì¢Œí‘œ: {len(coords)}ê°œ, ë¼ë²¨: {len(sampled_labels)}ê°œ, íŒ¨ë„ID: {len(panel_ids)}ê°œ")
        
        return {
            'coordinates': coords.tolist(),
            'panel_ids': panel_ids,
            'labels': sampled_labels
        }
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        logger.error(f"[UMAP ì˜¤ë¥˜] {str(e)}\n{error_trace}")
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"UMAP ê³„ì‚° ì‹¤íŒ¨: {str(e)}"
        )


class PanelClusterMappingRequest(BaseModel):
    """íŒ¨ë„ IDì™€ í´ëŸ¬ìŠ¤í„° ë§¤ì¹­ ìš”ì²­"""
    session_id: str
    panel_ids: List[str]


@router.post("/panel-cluster-mapping")
async def get_panel_cluster_mapping(req: PanelClusterMappingRequest):
    """
    ê²€ìƒ‰ëœ íŒ¨ë„ IDë“¤ì˜ í´ëŸ¬ìŠ¤í„° ë§¤í•‘ ì •ë³´ ë°˜í™˜
    """
    logger = logging.getLogger(__name__)
    
    try:
        logger.info(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] session_id: {req.session_id}, panel_ids: {len(req.panel_ids)}ê°œ")
        
        # Precomputed ë°ì´í„°ì¸ ê²½ìš° NeonDBì—ì„œ ë¡œë“œ
        if req.session_id == 'precomputed_default':
            logger.info(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] Precomputed ë°ì´í„° ì‚¬ìš© (NeonDB)")
            from app.utils.clustering_loader import get_precomputed_session_id, load_full_clustering_data_from_db
            
            # Precomputed ì„¸ì…˜ ID ì¡°íšŒ
            precomputed_name = "hdbscan_default"
            logger.debug(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] Precomputed ì„¸ì…˜ ID ì¡°íšŒ: name={precomputed_name}")
            actual_session_id = await get_precomputed_session_id(precomputed_name)
            
            if not actual_session_id:
                logger.error(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] Precomputed ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: name={precomputed_name}")
                raise HTTPException(
                    status_code=404,
                    detail=f"Precomputed ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. name={precomputed_name}"
                )
            
            logger.info(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] Precomputed ì„¸ì…˜ ID ì°¾ìŒ: {actual_session_id}")
            
            # ì „ì²´ í´ëŸ¬ìŠ¤í„°ë§ ë°ì´í„° ë¡œë“œ
            artifacts = await load_full_clustering_data_from_db(actual_session_id)
            if not artifacts:
                logger.error(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] NeonDBì—ì„œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: session_id={actual_session_id}")
                raise HTTPException(
                    status_code=404,
                    detail="Precomputed ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
            
            df = artifacts.get('data')
            labels_raw = artifacts.get('labels')
            
            if df is None or df.empty:
                raise HTTPException(
                    status_code=400,
                    detail="Precomputed ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
                )
            
            logger.info(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] NeonDBì—ì„œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}í–‰")
            
            # labels ì²˜ë¦¬
            if labels_raw is not None:
                if isinstance(labels_raw, np.ndarray):
                    labels = labels_raw.tolist()
                elif isinstance(labels_raw, list):
                    labels = labels_raw
                else:
                    labels = list(labels_raw)
            elif 'cluster' in df.columns:
                labels = df['cluster'].tolist()
            else:
                raise HTTPException(
                    status_code=400,
                    detail="í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
        else:
            # ì¼ë°˜ ì„¸ì…˜: artifactsì—ì„œ ë¡œë“œ
            from app.clustering.artifacts import load_artifacts
            artifacts = load_artifacts(req.session_id)
            
            if artifacts is None:
                logger.error(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {req.session_id}")
                raise HTTPException(
                    status_code=404,
                    detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
            
            df = artifacts.get('data')
            labels_raw = artifacts.get('labels')
            
            if df is None:
                raise HTTPException(
                    status_code=400,
                    detail="ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
                )
            
            # DataFrameì´ ê²½ë¡œ ë¬¸ìì—´ì´ë©´ ë¡œë“œ
            if isinstance(df, str):
                df = pd.read_csv(df)
            
            # labels ì²˜ë¦¬
            labels = None
            if labels_raw is not None:
                if isinstance(labels_raw, np.ndarray):
                    labels = labels_raw.tolist()
                elif isinstance(labels_raw, list):
                    labels = labels_raw
                else:
                    labels = list(labels_raw)
            
            if labels is None or len(labels) == 0:
                # DataFrameì— cluster ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
                if 'cluster' in df.columns:
                    labels = df['cluster'].tolist()
                else:
                    raise HTTPException(
                        status_code=400,
                        detail="í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    )
        
        # panel_ids ì¶”ì¶œ (mb_sn ì»¬ëŸ¼ ë˜ëŠ” ì¸ë±ìŠ¤)
        if 'mb_sn' in df.columns:
            df_panel_ids = df['mb_sn'].tolist()
        else:
            df_panel_ids = df.index.astype(str).tolist()
        
        logger.info(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] ë°ì´í„°í”„ë ˆì„ íŒ¨ë„ ìˆ˜: {len(df_panel_ids)}, ë ˆì´ë¸” ìˆ˜: {len(labels)}")
        
        # íŒ¨ë„ IDì™€ í´ëŸ¬ìŠ¤í„° ë§¤í•‘ ìƒì„± (ì •ê·œí™”ëœ í‚¤ë¡œ ì €ì¥)
        panel_to_cluster = {}
        for idx, panel_id in enumerate(df_panel_ids):
            if idx < len(labels):
                # ì •ê·œí™”: ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³  ê³µë°± ì œê±°
                normalized_id = str(panel_id).strip()
                panel_to_cluster[normalized_id] = int(labels[idx])
        
        logger.info(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] ë§¤í•‘ í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {len(panel_to_cluster)}ê°œ íŒ¨ë„")
        logger.info(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] ë§¤í•‘ í…Œì´ë¸” ìƒ˜í”Œ: {list(panel_to_cluster.items())[:5]}")
        logger.info(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] ìš”ì²­ëœ íŒ¨ë„ ID ìƒ˜í”Œ: {req.panel_ids[:5]}")
        
        # ë§¤í•‘ í…Œì´ë¸”ì˜ í‚¤ ìƒ˜í”Œ í™•ì¸ (ë””ë²„ê¹…)
        sample_keys = list(panel_to_cluster.keys())[:10]
        logger.debug(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] ë§¤í•‘ í…Œì´ë¸” í‚¤ ìƒ˜í”Œ: {sample_keys}")
        logger.debug(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] ë§¤í•‘ í…Œì´ë¸” í‚¤ íƒ€ì… ìƒ˜í”Œ: {[type(k).__name__ for k in sample_keys]}")
        
        # ìš”ì²­ëœ íŒ¨ë„ IDë“¤ì˜ í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¶”ì¶œ (ì •ê·œí™”í•˜ì—¬ ë§¤ì¹­)
        mapping_results = []
        not_found_ids = []
        for panel_id in req.panel_ids:
            # ìš”ì²­ëœ íŒ¨ë„ IDë„ ì •ê·œí™”
            normalized_request_id = str(panel_id).strip()
            cluster_id = panel_to_cluster.get(normalized_request_id, None)
            
            # ëŒ€ì†Œë¬¸ì ë¬´ì‹œ ë§¤ì¹­ ì‹œë„
            if cluster_id is None:
                for key, value in panel_to_cluster.items():
                    if str(key).strip().lower() == normalized_request_id.lower():
                        cluster_id = value
                        logger.debug(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] ëŒ€ì†Œë¬¸ì ë¬´ì‹œ ë§¤ì¹­ ì„±ê³µ: '{normalized_request_id}' -> '{key}'")
                        break
            
            if cluster_id is not None:
                mapping_results.append({
                    'panel_id': panel_id,
                    'cluster_id': cluster_id,
                    'found': True
                })
            else:
                # ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ì›ë³¸ IDì™€ ì •ê·œí™”ëœ ID ëª¨ë‘ ë¡œê·¸
                not_found_ids.append(normalized_request_id)
                logger.warning(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] íŒ¨ë„ ID ë§¤ì¹­ ì‹¤íŒ¨: ì›ë³¸='{panel_id}', ì •ê·œí™”='{normalized_request_id}'")
                mapping_results.append({
                    'panel_id': panel_id,
                    'cluster_id': None,
                    'found': False
                })
        
        if not_found_ids:
            logger.warning(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] ë§¤ì¹­ ì‹¤íŒ¨í•œ íŒ¨ë„ IDë“¤: {not_found_ids[:10]}")
            logger.warning(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] ë§¤í•‘ í…Œì´ë¸”ì— ìˆëŠ” ìœ ì‚¬í•œ í‚¤ë“¤: {[k for k in list(panel_to_cluster.keys())[:20] if any(nfid.lower() in str(k).lower() or str(k).lower() in nfid.lower() for nfid in not_found_ids[:3])]}")
        
        found_count = sum(1 for r in mapping_results if r['found'])
        logger.info(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘] ë§¤ì¹­ ì™„ë£Œ: {found_count}/{len(req.panel_ids)}ê°œ íŒ¨ë„ ì°¾ìŒ")
        
        return {
            'session_id': req.session_id,
            'mappings': mapping_results,
            'total_requested': len(req.panel_ids),
            'total_found': found_count
        }
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        logger.error(f"[íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘ ì˜¤ë¥˜] {str(e)}\n{error_trace}")
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"íŒ¨ë„-í´ëŸ¬ìŠ¤í„° ë§¤í•‘ ì‹¤íŒ¨: {str(e)}"
        )


# ë””ë²„ê·¸: íŒŒì¼ ë¡œë“œ ì™„ë£Œ í™•ì¸


# ============================================
# ê²€ìƒ‰ ê²°ê³¼ ì£¼ë³€ í´ëŸ¬ìŠ¤í„°ë§ ê´€ë ¨ í•¨ìˆ˜
# ============================================

# ì „ì—­ ìºì‹œ ë³€ìˆ˜
_cached_data = None
_cached_file_mtime = None
_cached_file_path = None


def load_full_data_cached(file_path: Optional[str] = None):
    """
    ì „ì²´ ë°ì´í„° ìºì‹± (íŒŒì¼ ìˆ˜ì • ì‹œê°„ ê¸°ë°˜)
    
    Parameters:
    -----------
    file_path : str, optional
        ë°ì´í„° íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©)
    
    Returns:
    --------
    tuple : (X_scaled, df, available_features, scaler)
        - X_scaled: ìŠ¤ì¼€ì¼ë§ëœ í”¼ì²˜ í–‰ë ¬
        - df: ì›ë³¸ DataFrame
        - available_features: ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ì²˜ ë¦¬ìŠ¤íŠ¸
        - scaler: StandardScaler ê°ì²´
    """
    global _cached_data, _cached_file_mtime, _cached_file_path
    
    if file_path is None:
        # CSV íŒŒì¼ ê¸°ë°˜ ë¡œë“œëŠ” ë” ì´ìƒ ì§€ì›í•˜ì§€ ì•ŠìŒ
        raise FileNotFoundError(
            "CSV íŒŒì¼ ê¸°ë°˜ ë°ì´í„° ë¡œë“œëŠ” ë” ì´ìƒ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. NeonDBì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì„¸ìš”."
        )
    
    file_path = Path(file_path)
    
    if not file_path.exists():
        raise FileNotFoundError(f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
    
    current_mtime = os.path.getmtime(file_path)
    
    # ìºì‹œê°€ ìˆê³  íŒŒì¼ì´ ë³€ê²½ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì¬ì‚¬ìš©
    if (_cached_data is not None and 
        _cached_file_path == str(file_path) and 
        _cached_file_mtime == current_mtime):
        logger = logging.getLogger(__name__)
        logger.debug(f"[ë°ì´í„° ìºì‹œ] ìºì‹œëœ ë°ì´í„° ì¬ì‚¬ìš©: {file_path}")
        return _cached_data
    
    # ìƒˆë¡œ ë¡œë“œ
    logger = logging.getLogger(__name__)
    logger.info(f"[ë°ì´í„° ë¡œë“œ] ìƒˆë¡œ ë¡œë“œ ì‹œì‘: {file_path}")
    
    df = pd.read_csv(file_path)
    
    # ì›ë³¸ í”¼ì²˜ ì‚¬ìš© (ìŠ¤ì¼€ì¼ë§ ì „)
    features = [
        'age',
        'Q6_income',  # ë˜ëŠ” 'Q6'
        'Q7',  # í•™ë ¥
        'Q8_count',
        'Q8_premium_index',
        'is_premium_car',
    ]
    
    # ì¡´ì¬í•˜ëŠ” í”¼ì²˜ë§Œ ì„ íƒ
    available_features = []
    for feat in features:
        if feat in df.columns:
            # ê²°ì¸¡ì¹˜ ë¹„ìœ¨ í™•ì¸
            missing_ratio = df[feat].isna().sum() / len(df)
            if missing_ratio <= 0.3:
                available_features.append(feat)
            else:
                logger.warning(f"[ë°ì´í„° ë¡œë“œ] í”¼ì²˜ {feat} ê²°ì¸¡ì¹˜ ë¹„ìœ¨ {missing_ratio:.1%} (ì œì™¸)")
        else:
            logger.warning(f"[ë°ì´í„° ë¡œë“œ] í”¼ì²˜ {feat} ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
    
    if len(available_features) < 3:
        raise ValueError(f"ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ì²˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤: {len(available_features)}ê°œ (ìµœì†Œ 3ê°œ í•„ìš”)")
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë° ìŠ¤ì¼€ì¼ë§
    X = df[available_features].fillna(df[available_features].mean()).values
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    logger.info(f"[ë°ì´í„° ë¡œë“œ] ì™„ë£Œ: {len(df)}í–‰, í”¼ì²˜ {len(available_features)}ê°œ")
    
    # ìºì‹œ ì—…ë°ì´íŠ¸
    _cached_data = (X_scaled, df, available_features, scaler)
    _cached_file_mtime = current_mtime
    _cached_file_path = str(file_path)
    
    return _cached_data


class ClusterAroundSearchRequest(BaseModel):
    """ê²€ìƒ‰ ê²°ê³¼ ì£¼ë³€ í´ëŸ¬ìŠ¤í„°ë§ ìš”ì²­"""
    search_panel_ids: List[str]
    k_neighbors_per_panel: int = 100  # ê° ê²€ìƒ‰ íŒ¨ë„ë‹¹ ì´ì›ƒ ìˆ˜


@router.post("/cluster-around-search")
async def cluster_around_search(req: ClusterAroundSearchRequest):
    """
    ê²€ìƒ‰ ê²°ê³¼ ì£¼ë³€ ìœ ì‚¬ íŒ¨ë„ ì¡°íšŒ (HDBSCAN ê²°ê³¼ ì¬ì‚¬ìš©)
    - Precomputed HDBSCAN ê²°ê³¼ì—ì„œ ê²€ìƒ‰ëœ íŒ¨ë„ì´ ì†í•œ í´ëŸ¬ìŠ¤í„° ì°¾ê¸°
    - í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì˜ ëª¨ë“  íŒ¨ë„ ë°˜í™˜ (ì¬í´ëŸ¬ìŠ¤í„°ë§ ì—†ìŒ)
    - Precomputed UMAP ì¢Œí‘œ ì¬ì‚¬ìš©
    """
    logger = logging.getLogger(__name__)
    
    try:
        logger.info("=" * 80)
        logger.info(f"[ğŸ” í™•ì¥ í´ëŸ¬ìŠ¤í„°ë§ API í˜¸ì¶œ]")
        logger.info(f"[ğŸ“‹ ìš”ì²­ ë°ì´í„°] ê²€ìƒ‰ íŒ¨ë„: {len(req.search_panel_ids)}ê°œ, ì´ì›ƒ ìˆ˜: {req.k_neighbors_per_panel}ê°œ")
        logger.info(f"[ğŸ“‹ ê²€ìƒ‰ íŒ¨ë„ ID ìƒ˜í”Œ] {req.search_panel_ids[:10]}")
        logger.info(f"[ğŸ“‹ ê²€ìƒ‰ íŒ¨ë„ ID íƒ€ì…] {[type(pid).__name__ for pid in req.search_panel_ids[:5]]}")
        logger.info("=" * 80)
        
        # 1. Precomputed HDBSCAN ë°ì´í„° ë¡œë“œ (NeonDBì—ì„œ ë¡œë“œ, df_full ì œê±°)
        logger.info(f"[2ë‹¨ê³„] Precomputed ë°ì´í„° ë¡œë“œ ì‹œì‘ (NeonDB)")
        from app.utils.clustering_loader import get_precomputed_session_id, load_full_clustering_data_from_db
        
        # Precomputed ì„¸ì…˜ ID ì¡°íšŒ
        precomputed_name = "hdbscan_default"
        actual_session_id = await get_precomputed_session_id(precomputed_name)
        
        if not actual_session_id:
            logger.error(f"[í™•ì¥ í´ëŸ¬ìŠ¤í„°ë§] Precomputed ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: name={precomputed_name}")
            raise HTTPException(
                status_code=404,
                detail=f"Precomputed ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. name={precomputed_name}"
            )
        
        logger.info(f"[í™•ì¥ í´ëŸ¬ìŠ¤í„°ë§] Precomputed ì„¸ì…˜ ID ì°¾ìŒ: {actual_session_id}")
        
        # ì „ì²´ í´ëŸ¬ìŠ¤í„°ë§ ë°ì´í„° ë¡œë“œ
        artifacts = await load_full_clustering_data_from_db(actual_session_id)
        if not artifacts:
            logger.error(f"[í™•ì¥ í´ëŸ¬ìŠ¤í„°ë§] NeonDBì—ì„œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: session_id={actual_session_id}")
            raise HTTPException(
                status_code=404,
                detail="Precomputed ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
        
        df_precomputed = artifacts.get('data')
        if df_precomputed is None or df_precomputed.empty:
            raise HTTPException(
                status_code=400,
                detail="Precomputed ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
            )
        
        logger.info(f"[í™•ì¥ í´ëŸ¬ìŠ¤í„°ë§] NeonDBì—ì„œ Precomputed ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df_precomputed)}í–‰")
        
        # Precomputed ë°ì´í„°ì—ì„œ íŒ¨ë„ ID ì»¬ëŸ¼ í™•ì¸
        panel_id_col = 'mb_sn' if 'mb_sn' in df_precomputed.columns else 'panel_id'
        
        if panel_id_col not in df_precomputed.columns:
            raise HTTPException(
                status_code=400,
                detail=f"Precomputed ë°ì´í„°ì— íŒ¨ë„ ID ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. (mb_sn ë˜ëŠ” panel_id í•„ìš”)"
            )
        
        # í´ëŸ¬ìŠ¤í„° ì»¬ëŸ¼ëª… í™•ì¸
        cluster_col = 'cluster'
        has_cluster_col = cluster_col in df_precomputed.columns
        
        # 3. ê²€ìƒ‰ íŒ¨ë„ì„ Precomputed ë°ì´í„°ì—ì„œ ì§ì ‘ ë§¤ì¹­ (df_full ì œê±°, NeonDB ë°ì´í„°ë§Œ ì‚¬ìš©)
        logger.info(f"[3ë‹¨ê³„] ê²€ìƒ‰ íŒ¨ë„ì„ Precomputed ë°ì´í„°ì—ì„œ ì§ì ‘ ë§¤ì¹­ ì‹œì‘")
        logger.info(f"  - ìš”ì²­ëœ íŒ¨ë„ ID ìˆ˜: {len(req.search_panel_ids)}ê°œ")
        logger.info(f"  - ìš”ì²­ëœ íŒ¨ë„ ID ìƒ˜í”Œ: {req.search_panel_ids[:5]}")
        logger.info(f"  - Precomputed ë°ì´í„° í–‰ ìˆ˜: {len(df_precomputed)}ê°œ")
        
        # Precomputed ë°ì´í„°ì˜ mb_snì„ ì •ê·œí™”í•˜ì—¬ ë§¤ì¹­ í…Œì´ë¸” ìƒì„±
        df_precomputed_normalized = df_precomputed.copy()
        df_precomputed_normalized['mb_sn_normalized'] = df_precomputed[panel_id_col].astype(str).str.strip().str.lower()
        precomputed_mb_sn_set = set(df_precomputed_normalized['mb_sn_normalized'].values)
        
        logger.info(f"[3ë‹¨ê³„] Precomputed ë§¤ì¹­ í…Œì´ë¸” ìƒì„± ì™„ë£Œ: {len(precomputed_mb_sn_set)}ê°œ ê³ ìœ  íŒ¨ë„ ID")
        logger.info(f"[3ë‹¨ê³„] Precomputed ìƒ˜í”Œ í‚¤: {list(precomputed_mb_sn_set)[:10]}")
        
        # ê²€ìƒ‰ íŒ¨ë„ ID ì •ê·œí™” ë° ë§¤ì¹­
        search_panel_mb_sns = set()
        not_found_panels = []
        found_panels = []
        
        for panel_id in req.search_panel_ids:
            panel_id_normalized = str(panel_id).strip().lower()
            
            if panel_id_normalized in precomputed_mb_sn_set:
                search_panel_mb_sns.add(panel_id_normalized)
                found_panels.append(panel_id)
                logger.debug(f"[âœ… 3ë‹¨ê³„] ë§¤ì¹­ ì„±ê³µ: '{panel_id}'")
            else:
                not_found_panels.append(panel_id)
                logger.debug(f"[âŒ 3ë‹¨ê³„] ë§¤ì¹­ ì‹¤íŒ¨: '{panel_id}'")
        
        logger.info(f"[3ë‹¨ê³„ ê²°ê³¼]")
        logger.info(f"  - ì°¾ì€ íŒ¨ë„: {len(found_panels)}ê°œ / {len(req.search_panel_ids)}ê°œ")
        logger.info(f"  - ì°¾ì§€ ëª»í•œ íŒ¨ë„: {len(not_found_panels)}ê°œ")
        if found_panels:
            logger.info(f"  - ì°¾ì€ íŒ¨ë„ ìƒ˜í”Œ: {found_panels[:5]}")
        if not_found_panels:
            logger.warning(f"  - ì°¾ì§€ ëª»í•œ íŒ¨ë„ ìƒ˜í”Œ: {not_found_panels[:5]}")
        
        # ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ì „ì²´ precomputed ë°ì´í„° ë°˜í™˜
        if len(search_panel_mb_sns) == 0:
            logger.warning(f"[âš ï¸ 3ë‹¨ê³„] ëª¨ë“  íŒ¨ë„ì„ ì°¾ì§€ ëª»í•¨ - ì „ì²´ precomputed ë°ì´í„° ë°˜í™˜")
            requested_set = set(str(pid).strip().lower() for pid in req.search_panel_ids)
            
            # ì „ì²´ precomputed ë°ì´í„° ë°˜í™˜ (í´ëŸ¬ìŠ¤í„°ë§ ì—†ì´)
            logger.info(f"[ì „ì²´ ë°ì´í„° ë°˜í™˜] precomputed UMAP ë°ì´í„° ì „ì²´ ë°˜í™˜")
            
            result_panels = []
            for _, row in df_precomputed.iterrows():
                panel_id = str(row[panel_id_col]).strip()
                panel_id_lower = panel_id.lower()
                is_search = panel_id_lower in requested_set
                
                # cluster ê°’ (ì—†ìœ¼ë©´ -1)
                cluster_value = int(row[cluster_col]) if has_cluster_col else -1
                
                result_panels.append({
                    'panel_id': panel_id,
                    'umap_x': float(row['umap_x']),
                    'umap_y': float(row['umap_y']),
                    'cluster': cluster_value,
                    'is_search_result': bool(is_search),
                    'original_cluster': cluster_value
                })
            
            # ì›ë³¸ í´ëŸ¬ìŠ¤í„° í†µê³„ (cluster ì»¬ëŸ¼ì´ ìˆì„ ë•Œë§Œ)
            cluster_stats = {}
            if has_cluster_col:
                for cluster_id in df_precomputed[cluster_col].unique():
                    cluster_mask = df_precomputed[cluster_col] == cluster_id
                    cluster_panels = [p for p in result_panels if p['cluster'] == cluster_id]
                    search_count = sum(1 for p in cluster_panels if p['is_search_result'])
                    
                    cluster_stats[int(cluster_id)] = {
                        'size': int(cluster_mask.sum()),
                        'percentage': float(cluster_mask.sum() / len(df_precomputed) * 100),
                        'search_count': search_count,
                        'search_percentage': float(search_count / max(1, cluster_mask.sum()) * 100)
                    }
            else:
                logger.warning(f"[ì „ì²´ ë°ì´í„° ë°˜í™˜] cluster ì»¬ëŸ¼ì´ ì—†ì–´ í´ëŸ¬ìŠ¤í„° í†µê³„ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            return {
                'success': True,
                'session_id': 'precomputed_default',
                'n_total_panels': len(result_panels),
                'n_search_panels': 0,
                'n_extended_panels': 0,
                'n_clusters': len(cluster_stats),
                'silhouette_score': None,
                'panels': result_panels,
                'cluster_stats': cluster_stats,
                'features_used': [],
                'dispersion_warning': False,
                'dispersion_ratio': 1.0,
                'warning': f'ê²€ìƒ‰ íŒ¨ë„ì„ í´ëŸ¬ìŠ¤í„°ë§ ë°ì´í„°ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ì–´ ì „ì²´ ë°ì´í„°ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤. ìš”ì²­ëœ {len(requested_set)}ê°œ ì¤‘ 0ê°œë§Œ ë°ì´í„°ì— ì¡´ì¬í•©ë‹ˆë‹¤.'
            }
        
        if len(not_found_panels) > 0:
            logger.warning(f"[âš ï¸ 3ë‹¨ê³„ ê²½ê³ ] {len(not_found_panels)}ê°œ íŒ¨ë„ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ê³„ì† ì§„í–‰)")
        
        logger.info(f"[âœ… 3ë‹¨ê³„ ì™„ë£Œ] ê²€ìƒ‰ íŒ¨ë„ ë§¤ì¹­ ì™„ë£Œ: {len(search_panel_mb_sns)}ê°œ")
        
        # 4. Precomputed HDBSCAN ê²°ê³¼ì—ì„œ ê²€ìƒ‰ëœ íŒ¨ë„ì´ ì†í•œ í´ëŸ¬ìŠ¤í„° ì°¾ê¸° (ì¬í´ëŸ¬ìŠ¤í„°ë§ ì—†ì´)
        logger.info(f"[4ë‹¨ê³„] HDBSCAN ê²°ê³¼ì—ì„œ ê²€ìƒ‰ëœ íŒ¨ë„ì˜ í´ëŸ¬ìŠ¤í„° ì°¾ê¸° (ì¬í´ëŸ¬ìŠ¤í„°ë§ ì—†ìŒ)")
        
        # Precomputed ë°ì´í„°ì—ì„œ ê²€ìƒ‰ëœ íŒ¨ë„ì´ ì†í•œ í´ëŸ¬ìŠ¤í„° ì°¾ê¸°
        has_cluster_col = cluster_col in df_precomputed.columns
        searched_cluster_ids = set()
        
        if has_cluster_col:
            for _, row in df_precomputed.iterrows():
                panel_id = str(row[panel_id_col]).strip().lower()
                if panel_id in search_panel_mb_sns:
                    cluster_id = int(row[cluster_col])
                    if cluster_id != -1:  # ë…¸ì´ì¦ˆ ì œì™¸
                        searched_cluster_ids.add(cluster_id)
        else:
            logger.warning(f"[4ë‹¨ê³„] cluster ì»¬ëŸ¼ì´ ì—†ì–´ í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ í™•ì¥ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        logger.info(f"[4ë‹¨ê³„ ì™„ë£Œ] ê²€ìƒ‰ëœ íŒ¨ë„ì´ ì†í•œ í´ëŸ¬ìŠ¤í„°: {sorted(searched_cluster_ids)}")
        
        # 5. í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì˜ ëª¨ë“  íŒ¨ë„ ì¶”ì¶œ (ì¬í´ëŸ¬ìŠ¤í„°ë§ ì—†ì´ HDBSCAN ê²°ê³¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
        extended_panel_ids = set()
        if has_cluster_col:
            for _, row in df_precomputed.iterrows():
                panel_id = str(row[panel_id_col]).strip()
                cluster_id = int(row[cluster_col])
                if cluster_id in searched_cluster_ids:
                    extended_panel_ids.add(panel_id.lower())
        else:
            # cluster ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ê²€ìƒ‰ëœ íŒ¨ë„ë§Œ í¬í•¨
            extended_panel_ids = search_panel_mb_sns.copy()
        
        logger.info(f"[HDBSCAN ê²°ê³¼ ì‚¬ìš©] ì¬í´ëŸ¬ìŠ¤í„°ë§ ì—†ì´ ê¸°ì¡´ HDBSCAN ê²°ê³¼ ì‚¬ìš©")
        logger.info(f"  - ê²€ìƒ‰ íŒ¨ë„: {len(search_panel_mb_sns)}ê°œ")
        logger.info(f"  - í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(searched_cluster_ids)}ê°œ")
        logger.info(f"  - í™•ì¥ íŒ¨ë„: {len(extended_panel_ids)}ê°œ")
        
        # 6. ê²°ê³¼ êµ¬ì„± (HDBSCAN ê²°ê³¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
        result_panels = []
        
        for _, row in df_precomputed.iterrows():
            panel_id = str(row[panel_id_col]).strip()
            panel_id_lower = panel_id.lower()
            cluster_id = int(row[cluster_col]) if has_cluster_col else -1
            
            # ê²€ìƒ‰ëœ íŒ¨ë„ì´ ì†í•œ í´ëŸ¬ìŠ¤í„°ì˜ íŒ¨ë„ë§Œ í¬í•¨ (ë˜ëŠ” cluster ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ê²€ìƒ‰ëœ íŒ¨ë„ë§Œ)
            if has_cluster_col:
                include_panel = cluster_id in searched_cluster_ids
            else:
                include_panel = panel_id_lower in search_panel_mb_sns
            
            if include_panel:
                is_search = panel_id_lower in search_panel_mb_sns
                
                result_panels.append({
                    'panel_id': panel_id,
                    'umap_x': float(row['umap_x']),
                    'umap_y': float(row['umap_y']),
                    'cluster': cluster_id,  # HDBSCAN í´ëŸ¬ìŠ¤í„° ID ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    'is_search_result': bool(is_search),
                    'original_cluster': cluster_id
                })
        
        # 7. í´ëŸ¬ìŠ¤í„°ë³„ í†µê³„ (HDBSCAN ê²°ê³¼ ê¸°ë°˜)
        cluster_stats = {}
        for cluster_id in searched_cluster_ids:
            cluster_panels = [p for p in result_panels if p['cluster'] == cluster_id]
            search_count = sum(1 for p in cluster_panels if p['is_search_result'])
            
            cluster_stats[int(cluster_id)] = {
                'size': len(cluster_panels),
                'percentage': float(len(cluster_panels) / len(result_panels) * 100) if result_panels else 0.0,
                'search_count': search_count,
                'search_percentage': float(search_count / max(1, len(cluster_panels)) * 100)
            }
        
        best_k = len(searched_cluster_ids)
        
        # 5. ì„¸ì…˜ ì •ë³´ì—ì„œ í’ˆì§ˆ ì§€í‘œ ê°€ì ¸ì˜¤ê¸° (NeonDBì—ì„œ)
        from app.utils.clustering_loader import load_clustering_session_from_db
        quality_metrics = {}
        try:
            session_data = await load_clustering_session_from_db(actual_session_id)
            if session_data:
                quality_metrics['silhouette_score'] = session_data.get('silhouette_score')
                quality_metrics['davies_bouldin_score'] = session_data.get('davies_bouldin_score')
                quality_metrics['calinski_harabasz_score'] = session_data.get('calinski_harabasz_score')
        except Exception as e:
            logger.warning(f"[í’ˆì§ˆ ì§€í‘œ ë¡œë“œ] ì‹¤íŒ¨: {str(e)}")
        
        logger.info(f"[HDBSCAN ê²°ê³¼ ì‚¬ìš© ì™„ë£Œ] ì„¸ì…˜ ID: {actual_session_id}, í´ëŸ¬ìŠ¤í„° ìˆ˜: {best_k} (ì¬í´ëŸ¬ìŠ¤í„°ë§ ì—†ìŒ)")
        
        return {
            'success': True,
            'session_id': actual_session_id,
            'n_total_panels': len(result_panels),
            'n_search_panels': len(search_panel_mb_sns),
            'n_extended_panels': len(extended_panel_ids) - len(search_panel_mb_sns),
            'n_clusters': best_k,
            'silhouette_score': quality_metrics.get('silhouette_score'),
            'davies_bouldin_score': quality_metrics.get('davies_bouldin_score'),
            'calinski_harabasz_score': quality_metrics.get('calinski_harabasz_score'),
            'panels': result_panels,
            'cluster_stats': cluster_stats,
            'features_used': [],  # Precomputed ë°ì´í„°ëŠ” í”¼ì²˜ ì •ë³´ ì—†ìŒ
            'dispersion_warning': False,
            'dispersion_ratio': 1.0,
            'method': 'HDBSCAN (precomputed from NeonDB, no re-clustering)',
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[í™•ì¥ í´ëŸ¬ìŠ¤í„°ë§ ì˜¤ë¥˜] {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"í™•ì¥ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {str(e)}"
        )
