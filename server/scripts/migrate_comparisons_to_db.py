"""
ë¹„êµ ë¶„ì„ ë°ì´í„° NeonDB ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸

ê¸°ì¡´ JSON íŒŒì¼ì— ì €ì¥ëœ ë¹„êµ ë¶„ì„ ë°ì´í„°ë¥¼ NeonDBë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜í•©ë‹ˆë‹¤.
- clustering_data/data/precomputed/comparison_results.json
"""
import asyncio
import os
import sys
import json
from pathlib import Path
from typing import Dict, Any
import logging
from dotenv import load_dotenv
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker
from sqlalchemy import text
import uuid

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(dotenv_path=project_root / '.env', override=True)
load_dotenv(dotenv_path=project_root / 'server' / '.env', override=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


async def migrate_comparisons_to_db(
    session: AsyncSession,
    session_id: str,
    comparison_results: Dict[str, Any]
) -> bool:
    """
    ë¹„êµ ë¶„ì„ ê²°ê³¼ë¥¼ NeonDBì— ì €ì¥
    
    Args:
        session: SQLAlchemy async session
        session_id: ì„¸ì…˜ ID (UUID ë¬¸ìì—´)
        comparison_results: ë¹„êµ ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ (pair_key -> comparison_data)
        
    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    try:
        logger.info(f"[ë¹„êµ ë¶„ì„ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘] session_id: {session_id}")
        logger.info(f"  ì´ ë¹„êµ ìŒ ìˆ˜: {len(comparison_results)}ê°œ")
        
        # ê¸°ì¡´ ë¹„êµ ë¶„ì„ ë°ì´í„° ì‚­ì œ (ê°™ì€ ì„¸ì…˜ì˜, merged ìŠ¤í‚¤ë§ˆ ì‚¬ìš©)
        await session.execute(
            text("DELETE FROM merged.cluster_comparisons WHERE session_id = :session_id"),
            {"session_id": session_id}
        )
        logger.info(f"  ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
        
        saved_count = 0
        error_count = 0
        
        for pair_key, comp_data in comparison_results.items():
            if 'error' in comp_data:
                error_count += 1
                logger.warning(f"  [ê±´ë„ˆë›°ê¸°] {pair_key}: ì˜¤ë¥˜ í¬í•¨")
                continue
            
            cluster_a = comp_data.get('cluster_a')
            cluster_b = comp_data.get('cluster_b')
            
            if cluster_a is None or cluster_b is None:
                error_count += 1
                logger.warning(f"  [ê±´ë„ˆë›°ê¸°] {pair_key}: cluster_a ë˜ëŠ” cluster_bê°€ ì—†ìŒ")
                continue
            
            # comparison_data êµ¬ì„± (compare_groups í•¨ìˆ˜ì˜ ë°˜í™˜ í˜•ì‹ê³¼ ë™ì¼)
            comparison_data = {
                'comparison': comp_data.get('comparison', []),
                'group_a': comp_data.get('group_a', {}),
                'group_b': comp_data.get('group_b', {}),
                'highlights': {
                    'num_top': [],
                    'bin_cat_top': []
                }
            }
            
            # í•˜ì´ë¼ì´íŠ¸ ê³„ì‚° (compare_groupsì—ì„œ ê³„ì‚°í•œ ê²ƒê³¼ ë™ì¼í•œ ë¡œì§)
            all_comparisons = comparison_data['comparison']
            
            # ì—°ì†í˜• ë³€ìˆ˜ í•˜ì´ë¼ì´íŠ¸ (cohens_d >= 0.3)
            continuous_comparisons = [
                c for c in all_comparisons 
                if c.get('type') == 'continuous' and abs(c.get('cohens_d', 0) or 0) >= 0.3
            ]
            continuous_sorted = sorted(
                continuous_comparisons,
                key=lambda x: abs(x.get('cohens_d', 0) or 0),
                reverse=True
            )[:5]
            
            # ì´ì§„í˜• ë³€ìˆ˜ í•˜ì´ë¼ì´íŠ¸ (abs_diff_pct >= 3.0 ë˜ëŠ” lift_pct >= 20.0)
            binary_comparisons = [
                c for c in all_comparisons 
                if c.get('type') == 'binary' and (
                    abs(c.get('abs_diff_pct', 0) or 0) >= 3.0 or
                    abs(c.get('lift_pct', 0) or 0) >= 20.0
                )
            ]
            binary_sorted = sorted(
                binary_comparisons,
                key=lambda x: abs(x.get('abs_diff_pct', 0) or 0),
                reverse=True
            )[:5]
            
            comparison_data['highlights'] = {
                'num_top': continuous_sorted,
                'bin_cat_top': binary_sorted
            }
            
            # DBì— ì‚½ì… (merged ìŠ¤í‚¤ë§ˆ ì‚¬ìš©)
            await session.execute(
                text("""
                    INSERT INTO merged.cluster_comparisons (
                        session_id, cluster_a, cluster_b, comparison_data
                    ) VALUES (
                        :session_id, :cluster_a, :cluster_b, CAST(:comparison_data AS jsonb)
                    )
                    ON CONFLICT (session_id, cluster_a, cluster_b) DO UPDATE SET
                        comparison_data = EXCLUDED.comparison_data,
                        updated_at = CURRENT_TIMESTAMP
                """),
                {
                    "session_id": session_id,
                    "cluster_a": cluster_a,
                    "cluster_b": cluster_b,
                    "comparison_data": json.dumps(comparison_data, ensure_ascii=False, default=str)
                }
            )
            saved_count += 1
            
            if saved_count % 20 == 0:
                logger.info(f"  ì§„í–‰ë¥ : {saved_count}/{len(comparison_results)}ê°œ ì €ì¥ ì™„ë£Œ")
        
        logger.info(f"[ë¹„êµ ë¶„ì„ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ] {saved_count}ê°œ ì„±ê³µ, {error_count}ê°œ ì‹¤íŒ¨")
        return True
        
    except Exception as e:
        logger.error(f"[ë¹„êµ ë¶„ì„ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨] ì˜¤ë¥˜: {str(e)}", exc_info=True)
        raise


async def main():
    """ë©”ì¸ ë§ˆì´ê·¸ë ˆì´ì…˜ í•¨ìˆ˜"""
    # ë°ì´í„°ë² ì´ìŠ¤ URI ê°€ì ¸ì˜¤ê¸°
    uri = os.getenv("ASYNC_DATABASE_URI")
    if not uri:
        logger.error("âŒ ASYNC_DATABASE_URI í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    if uri.startswith("postgresql://"):
        uri = uri.replace("postgresql://", "postgresql+psycopg://", 1)
    
    # ë¹„êµ ë¶„ì„ JSON íŒŒì¼ ê²½ë¡œ
    comparison_json_path = project_root / 'clustering_data' / 'data' / 'precomputed' / 'comparison_results.json'
    
    if not comparison_json_path.exists():
        logger.error(f"âŒ ë¹„êµ ë¶„ì„ JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {comparison_json_path}")
        return
    
    # JSON íŒŒì¼ ë¡œë“œ
    logger.info(f"[JSON íŒŒì¼ ë¡œë“œ] {comparison_json_path}")
    with open(comparison_json_path, 'r', encoding='utf-8') as f:
        json_data = json.load(f)
    
    comparison_results = json_data.get('comparisons', {})
    logger.info(f"[JSON ë¡œë“œ ì™„ë£Œ] {len(comparison_results)}ê°œ ë¹„êµ ìŒ")
    
    # Precomputed ì„¸ì…˜ ID ìƒì„± (hdbscan_default)
    precomputed_name = "hdbscan_default"
    session_uuid = uuid.uuid5(uuid.NAMESPACE_DNS, f"precomputed_{precomputed_name}")
    session_id = str(session_uuid)
    
    logger.info(f"[ì„¸ì…˜ ID] {session_id} (precomputed_name: {precomputed_name})")
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
    engine = create_async_engine(uri, echo=False)
    async_session = async_sessionmaker(engine, expire_on_commit=False)
    
    try:
        async with async_session() as session:
            async with session.begin():
                # ì„¸ì…˜ ì¡´ì¬ í™•ì¸ (merged ìŠ¤í‚¤ë§ˆ ì‚¬ìš©)
                result = await session.execute(
                    text("SELECT session_id FROM merged.clustering_sessions WHERE session_id = :session_id"),
                    {"session_id": session_id}
                )
                if result.scalar_one_or_none() is None:
                    logger.warning(f"âš ï¸ ì„¸ì…˜ì´ DBì— ì—†ìŠµë‹ˆë‹¤: {session_id}")
                    logger.info(f"ğŸ’¡ ë¨¼ì € Precomputed í´ëŸ¬ìŠ¤í„°ë§ ë°ì´í„°ë¥¼ ë§ˆì´ê·¸ë ˆì´ì…˜í•˜ì„¸ìš”:")
                    logger.info(f"   python server/scripts/migrate_clustering_to_db.py --precomputed")
                    return
                
                # ë¹„êµ ë¶„ì„ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜
                await migrate_comparisons_to_db(session, session_id, comparison_results)
        
        logger.info("âœ… ë¹„êµ ë¶„ì„ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {str(e)}", exc_info=True)
    finally:
        await engine.dispose()


if __name__ == "__main__":
    asyncio.run(main())

