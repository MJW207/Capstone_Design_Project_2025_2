{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChromaDB íŒ¨ë„ ê²€ìƒ‰ (ë©”íƒ€ë°ì´í„° í•„í„° ì ìš©)\n",
    "\n",
    "## ì£¼ìš” ê°œì„ ì‚¬í•­\n",
    "- **ë©”íƒ€ë°ì´í„° í•„í„° ì ìš©**: ì§€ì—­, ì—°ë ¹ëŒ€, ì„±ë³„ í•„í„°ë§\n",
    "- **Fallback ë©”ì»¤ë‹ˆì¦˜**: ë©”íƒ€ë°ì´í„° í•„í„°ë¡œ 0ê±´ì´ë©´ topicë§Œìœ¼ë¡œ ì¬ê²€ìƒ‰\n",
    "\n",
    "## ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸\n",
    "1. **ë©”íƒ€ë°ì´í„° ì¶”ì¶œ**: LLMìœ¼ë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ì—ì„œ êµ¬ì¡°í™”ëœ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ\n",
    "2. **ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜**: LLMìœ¼ë¡œ ë©”íƒ€ë°ì´í„°ë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜\n",
    "3. **í…ìŠ¤íŠ¸ ìƒì„±**: ì¹´í…Œê³ ë¦¬ë³„ë¡œ ìì—°ì–´ í…ìŠ¤íŠ¸ ìƒì„±\n",
    "4. **ì„ë² ë”© ìƒì„±**: Upstage Solarë¡œ ì„ë² ë”©\n",
    "5. **Topic + ë©”íƒ€ë°ì´í„° í•„í„°ë§ ê²€ìƒ‰**: ì¹´í…Œê³ ë¦¬(topic)ì™€ ë©”íƒ€ë°ì´í„°ë¡œ í•„í„°ë§\n",
    "6. **ë‹¨ê³„ì  í•„í„°ë§**: ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì ìš©í•˜ì—¬ í›„ë³´ ì¶•ì†Œ\n",
    "\n",
    "## í•„ìš”í•œ íŒ¨í‚¤ì§€\n",
    "```bash\n",
    "pip install anthropic langchain-upstage langchain-chroma chromadb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional\n",
    "from anthropic import Anthropic\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from collections import defaultdict\n",
    "\n",
    "# â­ íŒŒì¼ í•¸ë“¤ ì œí•œ í™•ì¸ ë° ì¡°ì • (WindowsëŠ” ìë™ ì¡°ì •ë¨)\n",
    "import sys\n",
    "if sys.platform != 'win32':\n",
    "    import resource\n",
    "    try:\n",
    "        soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)\n",
    "        print(f\"í˜„ì¬ íŒŒì¼ í•¸ë“¤ ì œí•œ: {soft} (ìµœëŒ€: {hard})\")\n",
    "        # ê°€ëŠ¥í•œ ìµœëŒ€ê°’ìœ¼ë¡œ ì„¤ì •\n",
    "        resource.setrlimit(resource.RLIMIT_NOFILE, (min(4096, hard), hard))\n",
    "        print(f\"ì¡°ì •ëœ íŒŒì¼ í•¸ë“¤ ì œí•œ: {min(4096, hard)}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ì„¤ì • ë° Config ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ\n",
      "   ì¹´í…Œê³ ë¦¬ ìˆ˜: 17ê°œ\n"
     ]
    }
   ],
   "source": [
    "# ChromaDB ì €ì¥ ê²½ë¡œ\n",
    "CHROMA_BASE_DIR = r\"C:\\Capstone\\Chroma_db\"\n",
    "\n",
    "# API Keys\n",
    "UPSTAGE_API_KEY = os.getenv('UPSTAGE_API_KEY', 'up_2KGGBmZpBmlePxUyk3ouWBf9iqOmJ')\n",
    "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY', 'sk-ant-api03-tV36n8KGbBMC_wIEwVCNVDnXQSCdYYF6ufLGPdpCoiVnmQH2lDf6b8WcyPafQdtJj9RlBRpKIoHGRO85F-W3DQ-6q97GQAA')\n",
    "\n",
    "# category_config.json ë¡œë“œ\n",
    "CATEGORY_CONFIG_PATH = r\"C:\\Capstone\\search2\\category_config.json\"\n",
    "\n",
    "with open(CATEGORY_CONFIG_PATH, 'r', encoding='utf-8') as f:\n",
    "    CATEGORY_CONFIG = json.load(f)\n",
    "\n",
    "print(\"âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ\")\n",
    "print(f\"   ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(CATEGORY_CONFIG)}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ë©”íƒ€ë°ì´í„° ì¶”ì¶œê¸° (LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MetadataExtractor í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "class MetadataExtractor:\n",
    "    \"\"\"LLMìœ¼ë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ\"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str):\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "        self.model = \"claude-haiku-4-5-20251001\"\n",
    "\n",
    "    def extract(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        ìì—°ì–´ ì¿¼ë¦¬ì—ì„œ êµ¬ì¡°í™”ëœ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ\n",
    "\n",
    "        Args:\n",
    "            query: ê²€ìƒ‰ ì¿¼ë¦¬ (ì˜ˆ: \"ì„œìš¸ 20ëŒ€ ë‚¨ì\")\n",
    "\n",
    "        Returns:\n",
    "            ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬ (ì˜ˆ: {\"ì§€ì—­\": \"ì„œìš¸\", \"ì—°ë ¹ëŒ€\": \"20ëŒ€\", \"ì„±ë³„\": \"ë‚¨\"})\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"ë‹¹ì‹ ì€ íŒ¨ë„ ê²€ìƒ‰ ì¿¼ë¦¬ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì‚¬ìš©ì ê²€ìƒ‰ ì¿¼ë¦¬: \"{query}\"\n",
    "\n",
    "ìœ„ ì¿¼ë¦¬ì—ì„œ ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ JSONìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”.\n",
    "\n",
    "ì¶”ì¶œ ê·œì¹™:\n",
    "1. ë‹¤ì¤‘ ê°’ ì§€ì›: \"ì„œìš¸, ê²½ê¸°\" â†’ {{\"ì§€ì—­\": [\"ì„œìš¸\", \"ê²½ê¸°\"]}}\n",
    "2. ë²”ìœ„ ë³€í™˜: \"20-30ëŒ€\" â†’ {{\"ì—°ë ¹ëŒ€\": [\"20ëŒ€\", \"30ëŒ€\"]}}\n",
    "3. ëª¨í˜¸í•œ í‘œí˜„ í•´ì„:\n",
    "   - \"ì Šì€ì¸µ/ì²­ë…„\" â†’ {{\"ì—°ë ¹ëŒ€\": [\"20ëŒ€\", \"30ëŒ€\"]}}\n",
    "   - \"MZì„¸ëŒ€\" â†’ {{\"ì—°ë ¹ëŒ€\": [\"20ëŒ€\", \"30ëŒ€\"]}}\n",
    "   - \"ì¤‘ë…„\" â†’ {{\"ì—°ë ¹ëŒ€\": [\"40ëŒ€\", \"50ëŒ€\"]}}\n",
    "   - \"ìˆ˜ë„ê¶Œ\" â†’ {{\"ì§€ì—­\": [\"ì„œìš¸\", \"ê²½ê¸°\", \"ì¸ì²œ\"]}}\n",
    "4. ì„±ë³„ ì •ê·œí™”: \"ë‚¨ì„±/ë‚¨ì/male\" â†’ \"ë‚¨\", \"ì—¬ì„±/ì—¬ì/female\" â†’ \"ì—¬\"\n",
    "5. í‚¤-ê°’ í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œ\n",
    "\n",
    "ì¶”ì¶œ ê°€ëŠ¥í•œ í•­ëª© (ìˆëŠ” ê²ƒë§Œ ì¶”ì¶œ):\n",
    "- ì§€ì—­, ì§€ì—­êµ¬, ë‚˜ì´, ì—°ë ¹ëŒ€, ì„±ë³„\n",
    "- ê²°í˜¼ì—¬ë¶€, ìë…€ìˆ˜, ê°€ì¡±ìˆ˜, í•™ë ¥\n",
    "- ì§ì—…, ì†Œë“, í™œë™, ì·¨ë¯¸, ê´€ì‹¬ì‚¬\n",
    "- ì „ìì œí’ˆ, ìë™ì°¨, í¡ì—°, ìŒì£¼\n",
    "- ê¸°íƒ€ íŠ¹ì„±\n",
    "\n",
    "JSONë§Œ ì¶œë ¥í•˜ì„¸ìš” (ì„¤ëª… ì—†ì´):\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=self.model,\n",
    "                max_tokens=1024,\n",
    "                temperature=0.0,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "\n",
    "            text = response.content[0].text\n",
    "            # JSON ì½”ë“œ ë¸”ë¡ ì œê±°\n",
    "            text = re.sub(r'^```json\\s*', '', text)\n",
    "            text = re.sub(r'\\s*```$', '', text)\n",
    "            text = text.strip()\n",
    "\n",
    "            metadata = json.loads(text)\n",
    "            print(f\"\\nğŸ“Š ì¶”ì¶œëœ ë©”íƒ€ë°ì´í„°: {metadata}\")\n",
    "            return metadata\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "            return {}\n",
    "\n",
    "\n",
    "print(\"âœ… MetadataExtractor í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ë©”íƒ€ë°ì´í„° í•„í„° ì¶”ì¶œê¸°\n",
    "\n",
    "ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì‚¬ìš©í•  ë©”íƒ€ë°ì´í„° í•„í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… extract_topic_filters í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def extract_topic_filters(metadata: Dict[str, Any], category: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    íŠ¹ì • ì¹´í…Œê³ ë¦¬ì— ì ìš©í•  ë©”íƒ€ë°ì´í„° í•„í„° ì¶”ì¶œ\n",
    "    \n",
    "    Args:\n",
    "        metadata: ì „ì²´ ë©”íƒ€ë°ì´í„°\n",
    "        category: ì¹´í…Œê³ ë¦¬ëª… (ì˜ˆ: \"ê¸°ë³¸ì •ë³´\")\n",
    "    \n",
    "    Returns:\n",
    "        í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— ì ìš©í•  í•„í„° (ì˜ˆ: {\"ì§€ì—­\": \"ì„œìš¸\", \"ì—°ë ¹ëŒ€\": \"20ëŒ€\", \"ì„±ë³„\": \"ë‚¨\"})\n",
    "    \"\"\"\n",
    "    # ì¹´í…Œê³ ë¦¬ë³„ ë©”íƒ€ë°ì´í„° ë§¤í•‘\n",
    "    CATEGORY_METADATA_MAPPING = {\n",
    "        \"ê¸°ë³¸ì •ë³´\": [\"ì§€ì—­\", \"ì§€ì—­êµ¬\", \"ì—°ë ¹ëŒ€\", \"ì„±ë³„\", \"ë‚˜ì´\", \"ê²°í˜¼ì—¬ë¶€\", \"ìë…€ìˆ˜\", \"ê°€ì¡±ìˆ˜\", \"í•™ë ¥\"],\n",
    "        \"ì§ì—…ì†Œë“\": [\"ì§ì—…\", \"ì†Œë“\"],\n",
    "        \"ê±´ê°•\": [\"í™œë™\", \"ìš´ë™\"],\n",
    "        # í•„ìš”ì‹œ ì¶”ê°€\n",
    "    }\n",
    "    \n",
    "    # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— ì ìš© ê°€ëŠ¥í•œ í‚¤ ì¶”ì¶œ\n",
    "    applicable_keys = CATEGORY_METADATA_MAPPING.get(category, [])\n",
    "    \n",
    "    filter_dict = {}\n",
    "    for key in applicable_keys:\n",
    "        if key in metadata:\n",
    "            value = metadata[key]\n",
    "            # ë¦¬ìŠ¤íŠ¸ë©´ ì²« ë²ˆì§¸ ê°’ ì‚¬ìš© (ë‹¤ì¤‘ ê°’ì€ OR ì¡°ê±´ìœ¼ë¡œ ì²˜ë¦¬ í•„ìš”í•˜ì§€ë§Œ ë‹¨ìˆœí™”)\n",
    "            if isinstance(value, list) and value:\n",
    "                filter_dict[key] = value[0]\n",
    "            elif value:\n",
    "                filter_dict[key] = value\n",
    "    \n",
    "    return filter_dict\n",
    "\n",
    "\n",
    "print(\"âœ… extract_topic_filters í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ê¸° (ê°„ì†Œí™”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CategoryClassifier í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "class CategoryClassifier:\n",
    "    \"\"\"ë©”íƒ€ë°ì´í„°ë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜ (ê·œì¹™ ê¸°ë°˜)\"\"\"\n",
    "\n",
    "    def __init__(self, category_config: Dict[str, Any]):\n",
    "        self.category_config = category_config\n",
    "\n",
    "    def classify(self, metadata: Dict[str, Any]) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        ë©”íƒ€ë°ì´í„°ë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜\n",
    "\n",
    "        Args:\n",
    "            metadata: ì¶”ì¶œëœ ë©”íƒ€ë°ì´í„°\n",
    "\n",
    "        Returns:\n",
    "            {\"ì¹´í…Œê³ ë¦¬ëª…\": [\"í‚¤1:ê°’1\", \"í‚¤2:ê°’2\"], ...}\n",
    "        \"\"\"\n",
    "        if not metadata:\n",
    "            return {}\n",
    "\n",
    "        result = defaultdict(list)\n",
    "\n",
    "        for key, value in metadata.items():\n",
    "            key_lower = key.lower()\n",
    "            matched = False\n",
    "\n",
    "            # ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ë§¤ì¹­\n",
    "            for cat_name, cat_info in self.category_config.items():\n",
    "                keywords = [k.lower() for k in cat_info.get('keywords', [])]\n",
    "                if key_lower in keywords or any(k in key_lower for k in keywords[:50]):\n",
    "                    if isinstance(value, list):\n",
    "                        for v in value:\n",
    "                            result[cat_name].append(f\"{key}:{v}\")\n",
    "                    else:\n",
    "                        result[cat_name].append(f\"{key}:{value}\")\n",
    "                    matched = True\n",
    "                    break\n",
    "\n",
    "            # ë§¤ì¹­ ì•ˆë˜ë©´ ê¸°ë³¸ì •ë³´ë¡œ\n",
    "            if not matched:\n",
    "                if isinstance(value, list):\n",
    "                    for v in value:\n",
    "                        result[\"ê¸°ë³¸ì •ë³´\"].append(f\"{key}:{v}\")\n",
    "                else:\n",
    "                    result[\"ê¸°ë³¸ì •ë³´\"].append(f\"{key}:{value}\")\n",
    "\n",
    "        print(f\"\\nğŸ·ï¸  ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ê²°ê³¼: {dict(result)}\")\n",
    "        return dict(result)\n",
    "\n",
    "\n",
    "print(\"âœ… CategoryClassifier í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. í…ìŠ¤íŠ¸ ìƒì„±ê¸° (LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CategoryTextGenerator í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "class CategoryTextGenerator:\n",
    "    \"\"\"ì¹´í…Œê³ ë¦¬ë³„ë¡œ ìì—°ì–´ í…ìŠ¤íŠ¸ ìƒì„±\"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str):\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "        self.model = \"claude-haiku-4-5-20251001\"\n",
    "\n",
    "    def generate(self, category: str, metadata_items: List[str]) -> str:\n",
    "        \"\"\"ì¹´í…Œê³ ë¦¬ë³„ ìì—°ì–´ í…ìŠ¤íŠ¸ ìƒì„±\"\"\"\n",
    "        if not metadata_items:\n",
    "            return \"\"\n",
    "\n",
    "        metadata_str = \"\\n\".join([f\"- {item}\" for item in metadata_items])\n",
    "\n",
    "        prompt = f\"\"\"ë©”íƒ€ë°ì´í„°ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”.\n",
    "\n",
    "ì¹´í…Œê³ ë¦¬: {category}\n",
    "ë©”íƒ€ë°ì´í„°:\n",
    "{metadata_str}\n",
    "\n",
    "ê·œì¹™:\n",
    "1. ì¡´ëŒ“ë§ ì‚¬ìš© (ì…ë‹ˆë‹¤/ìŠµë‹ˆë‹¤ ì²´)\n",
    "2. ì¹´í…Œê³ ë¦¬ ì´ë¦„ì€ í¬í•¨í•˜ì§€ ë§ ê²ƒ\n",
    "3. ìì—°ìŠ¤ëŸ¬ìš´ í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±\n",
    "\n",
    "ë¬¸ì¥ë§Œ ì¶œë ¥:\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=self.model,\n",
    "                max_tokens=512,\n",
    "                temperature=0.3,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "\n",
    "            text = response.content[0].text.strip()\n",
    "            print(f\"\\nğŸ“ [{category}] ìƒì„±: {text[:100]}...\")\n",
    "            return text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ [{category}] í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "\n",
    "print(\"âœ… CategoryTextGenerator í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ì„ë² ë”© ìƒì„±ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… EmbeddingGenerator í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingGenerator:\n",
    "    \"\"\"Upstage Solarë¡œ ì„ë² ë”© ìƒì„±\"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str):\n",
    "        self.embeddings = UpstageEmbeddings(\n",
    "            api_key=api_key,\n",
    "            model=\"solar-embedding-1-large-query\"\n",
    "        )\n",
    "\n",
    "    def generate(self, texts: Dict[str, str]) -> Dict[str, List[float]]:\n",
    "        \"\"\"ì¹´í…Œê³ ë¦¬ë³„ ì„ë² ë”© ìƒì„±\"\"\"\n",
    "        result = {}\n",
    "\n",
    "        for category, text in texts.items():\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                embedding = self.embeddings.embed_query(text)\n",
    "                result[category] = embedding\n",
    "                print(f\"âœ… [{category}] ì„ë² ë”© ìƒì„± ì™„ë£Œ\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ [{category}] ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "print(\"âœ… EmbeddingGenerator í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ChromaDB ê²€ìƒ‰ê¸° (Topic + ë©”íƒ€ë°ì´í„° í•„í„°ë§) â­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ChromaPanelSearcher í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ (ì—„ê²©í•œ í•„í„°ë§ + ì—°ê²° ê´€ë¦¬)\n"
     ]
    }
   ],
   "source": [
    "class ChromaPanelSearcher:\n",
    "    \"\"\"ChromaDBì—ì„œ topic + ë©”íƒ€ë°ì´í„° í•„í„°ë§ ê²€ìƒ‰ (ì—„ê²©í•œ í•„í„°ë§)\"\"\"\n",
    "\n",
    "    def __init__(self, chroma_base_dir: str, category_config: Dict[str, Any], upstage_api_key: str):\n",
    "        self.chroma_base_dir = chroma_base_dir\n",
    "        self.category_config = category_config\n",
    "        self.embeddings = UpstageEmbeddings(\n",
    "            api_key=upstage_api_key,\n",
    "            model=\"solar-embedding-1-large\"\n",
    "        )\n",
    "\n",
    "    def get_available_panels(self) -> List[str]:\n",
    "        \"\"\"ì‚¬ìš© ê°€ëŠ¥í•œ íŒ¨ë„ ëª©ë¡\"\"\"\n",
    "        if not os.path.exists(self.chroma_base_dir):\n",
    "            return []\n",
    "\n",
    "        panels = []\n",
    "        for item in os.listdir(self.chroma_base_dir):\n",
    "            full_path = os.path.join(self.chroma_base_dir, item)\n",
    "            if os.path.isdir(full_path) and item.startswith(\"panel_\"):\n",
    "                mb_sn = item.replace(\"panel_\", \"\").replace(\"_\", \"-\")\n",
    "                panels.append(mb_sn)\n",
    "\n",
    "        return panels\n",
    "\n",
    "    def _validate_metadata(self, doc_metadata: Dict[str, Any], required_filters: Dict[str, Any]) -> bool:\n",
    "        \"\"\"\n",
    "        ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ê°€ í•„í„° ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ”ì§€ ê²€ì¦\n",
    "        \n",
    "        Args:\n",
    "            doc_metadata: ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°\n",
    "            required_filters: í•„ìˆ˜ í•„í„° ì¡°ê±´\n",
    "            \n",
    "        Returns:\n",
    "            ëª¨ë“  ì¡°ê±´ì„ ë§Œì¡±í•˜ë©´ True, ì•„ë‹ˆë©´ False\n",
    "        \"\"\"\n",
    "        for key, expected_value in required_filters.items():\n",
    "            actual_value = doc_metadata.get(key, '')\n",
    "            \n",
    "            # ë¹ˆ ë¬¸ìì—´ì´ë©´ ì¡°ê±´ ë¶ˆë§Œì¡±\n",
    "            if not actual_value or actual_value == '':\n",
    "                return False\n",
    "            \n",
    "            # ê°’ì´ ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ ì¡°ê±´ ë¶ˆë§Œì¡±\n",
    "            if str(actual_value) != str(expected_value):\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def search_by_category(\n",
    "        self,\n",
    "        mb_sn: str,\n",
    "        category: str,\n",
    "        query_embedding: List[float],\n",
    "        metadata_filter: Dict[str, Any] = None,\n",
    "        top_k: int = 5\n",
    "    ) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        íŠ¹ì • íŒ¨ë„ì˜ íŠ¹ì • ì¹´í…Œê³ ë¦¬ì—ì„œ ê²€ìƒ‰ (ë©”íƒ€ë°ì´í„° í•„í„° ì ìš©)\n",
    "\n",
    "        Args:\n",
    "            mb_sn: íŒ¨ë„ ID\n",
    "            category: ì¹´í…Œê³ ë¦¬ëª… (ì˜ˆ: \"ê¸°ë³¸ì •ë³´\")\n",
    "            query_embedding: ì¿¼ë¦¬ ì„ë² ë”©\n",
    "            metadata_filter: ë©”íƒ€ë°ì´í„° í•„í„° (ì˜ˆ: {\"ì§€ì—­\": \"ì„œìš¸\", \"ì—°ë ¹ëŒ€\": \"20ëŒ€\", \"ì„±ë³„\": \"ë‚¨\"})\n",
    "            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜\n",
    "\n",
    "        Returns:\n",
    "            ê²€ìƒ‰ ê²°ê³¼ ë˜ëŠ” None\n",
    "        \"\"\"\n",
    "        # ì¹´í…Œê³ ë¦¬ â†’ topic ë§¤í•‘\n",
    "        pinecone_topic = self.category_config.get(category, {}).get(\"pinecone_topic\", category)\n",
    "\n",
    "        collection_name = f\"panel_{mb_sn}\".replace(\"-\", \"_\")\n",
    "        persist_directory = os.path.join(self.chroma_base_dir, collection_name)\n",
    "\n",
    "        if not os.path.exists(persist_directory):\n",
    "            return None\n",
    "\n",
    "        vectorstore = None\n",
    "        try:\n",
    "            vectorstore = Chroma(\n",
    "                collection_name=collection_name,\n",
    "                embedding_function=self.embeddings,\n",
    "                persist_directory=persist_directory\n",
    "            )\n",
    "\n",
    "            # â­ ë©”íƒ€ë°ì´í„° í•„í„° ì ìš© (ChromaDB $and ì—°ì‚°ì)\n",
    "            if metadata_filter:\n",
    "                filter_conditions = [{\"topic\": pinecone_topic}]\n",
    "                for key, value in metadata_filter.items():\n",
    "                    if value:  # ë¹ˆê°’ ì•„ë‹Œ ê²½ìš°ë§Œ í•„í„° ì¶”ê°€\n",
    "                        filter_conditions.append({key: value})\n",
    "                where_filter = {\"$and\": filter_conditions}\n",
    "            else:\n",
    "                where_filter = {\"topic\": pinecone_topic}\n",
    "\n",
    "            # ë” ë§ì€ í›„ë³´ ê²€ìƒ‰ (í•„í„°ë§ í›„ top_kê°œ ë‚¨ê¸°ê¸° ìœ„í•´)\n",
    "            results = vectorstore.similarity_search_by_vector_with_relevance_scores(\n",
    "                embedding=query_embedding,\n",
    "                k=top_k * 3,  # 3ë°°ìˆ˜ë¡œ ê²€ìƒ‰\n",
    "                filter=where_filter\n",
    "            )\n",
    "\n",
    "            if not results:\n",
    "                return None\n",
    "\n",
    "            # â­ í•µì‹¬: ê²€ìƒ‰ ê²°ê³¼ë¥¼ í›„ì²˜ë¦¬í•˜ì—¬ ë©”íƒ€ë°ì´í„° ê²€ì¦\n",
    "            valid_results = []\n",
    "            if metadata_filter:\n",
    "                for doc, score in results:\n",
    "                    # ë©”íƒ€ë°ì´í„°ê°€ í•„í„° ì¡°ê±´ì„ ì •í™•íˆ ë§Œì¡±í•˜ëŠ”ì§€ ê²€ì¦\n",
    "                    if self._validate_metadata(doc.metadata, metadata_filter):\n",
    "                        valid_results.append((doc, score))\n",
    "                    \n",
    "                    if len(valid_results) >= top_k:\n",
    "                        break\n",
    "            else:\n",
    "                valid_results = results[:top_k]\n",
    "\n",
    "            if not valid_results:\n",
    "                return None\n",
    "\n",
    "            best_doc, best_score = valid_results[0]\n",
    "\n",
    "            return {\n",
    "                \"mb_sn\": mb_sn,\n",
    "                \"category\": category,\n",
    "                \"topic\": best_doc.metadata.get(\"topic\"),\n",
    "                \"score\": float(best_score),\n",
    "                \"metadata\": best_doc.metadata,\n",
    "                \"text\": best_doc.page_content[:200]\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return None\n",
    "        finally:\n",
    "            # â­ í•µì‹¬: vectorstore ëª…ì‹œì ìœ¼ë¡œ ë‹«ê¸°\n",
    "            if vectorstore is not None:\n",
    "                try:\n",
    "                    # ChromaDB í´ë¼ì´ì–¸íŠ¸ì˜ ë‚´ë¶€ HTTP ì—°ê²° ë‹«ê¸°\n",
    "                    if hasattr(vectorstore, '_client'):\n",
    "                        vectorstore._client = None\n",
    "                    del vectorstore\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "\n",
    "print(\"âœ… ChromaPanelSearcher í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ (ì—„ê²©í•œ í•„í„°ë§ + ì—°ê²° ê´€ë¦¬)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ê²°ê³¼ í•„í„° (ë‹¨ê³„ì  í•„í„°ë§)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ResultFilter í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ (ë³‘ë ¬ ê²€ìƒ‰ + ì—°ê²° ê´€ë¦¬)\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import gc\n",
    "\n",
    "class ResultFilter:\n",
    "    \"\"\"ë‹¨ê³„ì  í•„í„°ë§ìœ¼ë¡œ ìµœì¢… í›„ë³´ ì„ ë³„ (ë³‘ë ¬ ê²€ìƒ‰ ìµœì í™”)\"\"\"\n",
    "\n",
    "    def __init__(self, searcher: ChromaPanelSearcher, max_workers: int = 5):  # â­ 20 â†’ 5ë¡œ ê°ì†Œ\n",
    "        self.searcher = searcher\n",
    "        self.max_workers = max_workers\n",
    "\n",
    "    def filter_by_categories(\n",
    "        self,\n",
    "        available_panels: List[str],\n",
    "        category_embeddings: Dict[str, List[float]],\n",
    "        category_filters: Dict[str, Dict[str, Any]],\n",
    "        category_order: List[str],\n",
    "        final_count: int = 10\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        ë‹¨ê³„ì  í•„í„°ë§ (ë³‘ë ¬ ê²€ìƒ‰)\n",
    "\n",
    "        Args:\n",
    "            available_panels: ê²€ìƒ‰ ëŒ€ìƒ íŒ¨ë„ ë¦¬ìŠ¤íŠ¸\n",
    "            category_embeddings: ì¹´í…Œê³ ë¦¬ë³„ ì„ë² ë”©\n",
    "            category_filters: ì¹´í…Œê³ ë¦¬ë³„ ë©”íƒ€ë°ì´í„° í•„í„°\n",
    "            category_order: ì¹´í…Œê³ ë¦¬ ì ìš© ìˆœì„œ\n",
    "            final_count: ìµœì¢… ë°˜í™˜ ê°œìˆ˜\n",
    "\n",
    "        Returns:\n",
    "            ìµœì¢… ì„ ë³„ëœ mb_sn ë¦¬ìŠ¤íŠ¸\n",
    "        \"\"\"\n",
    "        print(f\"\\në‹¨ê³„ì  í•„í„°ë§ ì‹œì‘ (ë³‘ë ¬ ê²€ìƒ‰ í™œì„±í™”, workers={self.max_workers})\")\n",
    "        print(f\"   ì´ˆê¸° í›„ë³´: {len(available_panels)}ê°œ\")\n",
    "        print(f\"   ì¹´í…Œê³ ë¦¬ ìˆœì„œ: {category_order}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        # 1ë‹¨ê³„: ì²« ë²ˆì§¸ ì¹´í…Œê³ ë¦¬ë¡œ ì´ˆê¸° í›„ë³´ ì„ ë³„ (ë³‘ë ¬ ê²€ìƒ‰)\n",
    "        first_category = category_order[0]\n",
    "        first_embedding = category_embeddings[first_category]\n",
    "        first_filter = category_filters.get(first_category, {})\n",
    "\n",
    "        print(f\"\\n[1ë‹¨ê³„] {first_category} ì¹´í…Œê³ ë¦¬ë¡œ ê²€ìƒ‰ (ë³‘ë ¬)\")\n",
    "        print(f\"   ë©”íƒ€ë°ì´í„° í•„í„°: {first_filter}\")\n",
    "        \n",
    "        candidate_scores = {}\n",
    "\n",
    "        # ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            future_to_mb_sn = {\n",
    "                executor.submit(\n",
    "                    self.searcher.search_by_category,\n",
    "                    mb_sn,\n",
    "                    first_category,\n",
    "                    first_embedding,\n",
    "                    first_filter,\n",
    "                    1\n",
    "                ): mb_sn\n",
    "                for mb_sn in available_panels\n",
    "            }\n",
    "\n",
    "            for future in as_completed(future_to_mb_sn):\n",
    "                mb_sn = future_to_mb_sn[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if result:\n",
    "                        candidate_scores[mb_sn] = result[\"score\"]\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "\n",
    "        # â­ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "        gc.collect()\n",
    "\n",
    "        candidates = sorted(candidate_scores.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "        candidate_mb_sns = [mb_sn for mb_sn, _ in candidates]\n",
    "\n",
    "        print(f\"   -> {len(candidate_mb_sns)}ê°œ í›„ë³´ ì„ ë³„\")\n",
    "\n",
    "        # 2ë‹¨ê³„ ì´í›„: ìˆœì°¨ì ìœ¼ë¡œ í•„í„°ë§ (ë³‘ë ¬ ê²€ìƒ‰)\n",
    "        for step, category in enumerate(category_order[1:], 2):\n",
    "            if category not in category_embeddings:\n",
    "                continue\n",
    "\n",
    "            print(f\"\\n[{step}ë‹¨ê³„] {category} ì¹´í…Œê³ ë¦¬ë¡œ í•„í„°ë§ (ë³‘ë ¬)\")\n",
    "            embedding = category_embeddings[category]\n",
    "            cat_filter = category_filters.get(category, {})\n",
    "            print(f\"   ë©”íƒ€ë°ì´í„° í•„í„°: {cat_filter}\")\n",
    "\n",
    "            new_scores = {}\n",
    "\n",
    "            # ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰\n",
    "            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "                future_to_mb_sn = {\n",
    "                    executor.submit(\n",
    "                        self.searcher.search_by_category,\n",
    "                        mb_sn,\n",
    "                        category,\n",
    "                        embedding,\n",
    "                        cat_filter,\n",
    "                        1\n",
    "                    ): mb_sn\n",
    "                    for mb_sn in candidate_mb_sns\n",
    "                }\n",
    "\n",
    "                for future in as_completed(future_to_mb_sn):\n",
    "                    mb_sn = future_to_mb_sn[future]\n",
    "                    try:\n",
    "                        result = future.result()\n",
    "                        if result:\n",
    "                            prev_score = candidate_scores.get(mb_sn, 0)\n",
    "                            new_scores[mb_sn] = prev_score + result[\"score\"]\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "\n",
    "            # â­ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜\n",
    "            gc.collect()\n",
    "\n",
    "            candidates = sorted(new_scores.items(), key=lambda x: x[1], reverse=True)[:final_count * 3]\n",
    "            candidate_mb_sns = [mb_sn for mb_sn, _ in candidates]\n",
    "            candidate_scores = dict(candidates)\n",
    "\n",
    "            print(f\"   -> {len(candidate_mb_sns)}ê°œ í›„ë³´ë¡œ ì¶•ì†Œ\")\n",
    "\n",
    "        final_mb_sns = candidate_mb_sns[:final_count]\n",
    "\n",
    "        print(f\"\\nìµœì¢… {len(final_mb_sns)}ê°œ íŒ¨ë„ ì„ ë³„ ì™„ë£Œ\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        return final_mb_sns\n",
    "\n",
    "\n",
    "print(\"âœ… ResultFilter í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ (ë³‘ë ¬ ê²€ìƒ‰ + ì—°ê²° ê´€ë¦¬)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ì „ì²´ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PanelSearchPipeline í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "class PanelSearchPipeline:\n",
    "    \"\"\"ì „ì²´ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ (ë©”íƒ€ë°ì´í„° í•„í„° ì ìš©)\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        chroma_base_dir: str,\n",
    "        category_config: Dict[str, Any],\n",
    "        anthropic_api_key: str,\n",
    "        upstage_api_key: str\n",
    "    ):\n",
    "        self.metadata_extractor = MetadataExtractor(anthropic_api_key)\n",
    "        self.category_classifier = CategoryClassifier(category_config)\n",
    "        self.text_generator = CategoryTextGenerator(anthropic_api_key)\n",
    "        self.embedding_generator = EmbeddingGenerator(upstage_api_key)\n",
    "        self.searcher = ChromaPanelSearcher(chroma_base_dir, category_config, upstage_api_key)\n",
    "        self.result_filter = ResultFilter(self.searcher)\n",
    "\n",
    "    def search(self, query: str, top_k: int = 10) -> List[str]:\n",
    "        \"\"\"\n",
    "        ìì—°ì–´ ì¿¼ë¦¬ë¡œ íŒ¨ë„ ê²€ìƒ‰\n",
    "\n",
    "        Args:\n",
    "            query: ê²€ìƒ‰ ì¿¼ë¦¬ (ì˜ˆ: \"ì„œìš¸ 20ëŒ€ ë‚¨ì\")\n",
    "            top_k: ë°˜í™˜í•  íŒ¨ë„ ìˆ˜\n",
    "\n",
    "        Returns:\n",
    "            mb_sn ë¦¬ìŠ¤íŠ¸\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: '{query}'\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        # 1ë‹¨ê³„: ë©”íƒ€ë°ì´í„° ì¶”ì¶œ\n",
    "        print(\"\\n[1ë‹¨ê³„] ë©”íƒ€ë°ì´í„° ì¶”ì¶œ\")\n",
    "        metadata = self.metadata_extractor.extract(query)\n",
    "\n",
    "        if not metadata:\n",
    "            print(\"âŒ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨\")\n",
    "            return []\n",
    "\n",
    "        # 2ë‹¨ê³„: ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜\n",
    "        print(\"\\n[2ë‹¨ê³„] ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜\")\n",
    "        classified = self.category_classifier.classify(metadata)\n",
    "\n",
    "        if not classified:\n",
    "            print(\"âŒ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì‹¤íŒ¨\")\n",
    "            return []\n",
    "\n",
    "        # 2.5ë‹¨ê³„: ì¹´í…Œê³ ë¦¬ë³„ ë©”íƒ€ë°ì´í„° í•„í„° ì¶”ì¶œ\n",
    "        print(\"\\n[2.5ë‹¨ê³„] ì¹´í…Œê³ ë¦¬ë³„ ë©”íƒ€ë°ì´í„° í•„í„° ì¶”ì¶œ\")\n",
    "        category_filters = {}\n",
    "        for category in classified.keys():\n",
    "            cat_filter = extract_topic_filters(metadata, category)\n",
    "            if cat_filter:\n",
    "                category_filters[category] = cat_filter\n",
    "                print(f\"   [{category}] í•„í„°: {cat_filter}\")\n",
    "\n",
    "        # 3ë‹¨ê³„: ìì—°ì–´ í…ìŠ¤íŠ¸ ìƒì„±\n",
    "        print(\"\\n[3ë‹¨ê³„] ìì—°ì–´ í…ìŠ¤íŠ¸ ìƒì„±\")\n",
    "        texts = {}\n",
    "        for category, items in classified.items():\n",
    "            text = self.text_generator.generate(category, items)\n",
    "            if text:\n",
    "                texts[category] = text\n",
    "\n",
    "        # 4ë‹¨ê³„: ì„ë² ë”© ìƒì„±\n",
    "        print(\"\\n[4ë‹¨ê³„] ì„ë² ë”© ìƒì„±\")\n",
    "        embeddings = self.embedding_generator.generate(texts)\n",
    "\n",
    "        if not embeddings:\n",
    "            print(\"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨\")\n",
    "            return []\n",
    "\n",
    "        # 5ë‹¨ê³„: ë‹¨ê³„ì  í•„í„°ë§ ê²€ìƒ‰\n",
    "        available_panels = self.searcher.get_available_panels()\n",
    "        print(f\"\\n[5ë‹¨ê³„] ê²€ìƒ‰ ê°€ëŠ¥í•œ íŒ¨ë„: {len(available_panels)}ê°œ\")\n",
    "\n",
    "        category_order = list(embeddings.keys())\n",
    "        final_mb_sns = self.result_filter.filter_by_categories(\n",
    "            available_panels=available_panels,\n",
    "            category_embeddings=embeddings,\n",
    "            category_filters=category_filters,\n",
    "            category_order=category_order,\n",
    "            final_count=top_k\n",
    "        )\n",
    "\n",
    "        return final_mb_sns\n",
    "\n",
    "\n",
    "print(\"âœ… PanelSearchPipeline í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SUCCESS] ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”\n",
    "try:\n",
    "    pipeline = PanelSearchPipeline(\n",
    "        chroma_base_dir=CHROMA_BASE_DIR,\n",
    "        category_config=CATEGORY_CONFIG,\n",
    "        anthropic_api_key=ANTHROPIC_API_KEY,\n",
    "        upstage_api_key=UPSTAGE_API_KEY\n",
    "    )\n",
    "    print(\"\\n[SUCCESS] ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨:\")\n",
    "    print(f\"  ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}\")\n",
    "    print(f\"  ì˜¤ë¥˜ ë©”ì‹œì§€: {str(e)}\")\n",
    "    import traceback\n",
    "    print(f\"\\nìƒì„¸ ì˜¤ë¥˜:\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. í…ŒìŠ¤íŠ¸: ê²€ìƒ‰ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: 'ê²½ê¸° 30ëŒ€ ì—¬ì'\n",
      "================================================================================\n",
      "\n",
      "[1ë‹¨ê³„] ë©”íƒ€ë°ì´í„° ì¶”ì¶œ\n",
      "\n",
      "ğŸ“Š ì¶”ì¶œëœ ë©”íƒ€ë°ì´í„°: {'ì§€ì—­': ['ê²½ê¸°'], 'ì—°ë ¹ëŒ€': ['30ëŒ€'], 'ì„±ë³„': 'ì—¬'}\n",
      "\n",
      "[2ë‹¨ê³„] ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜\n",
      "\n",
      "ğŸ·ï¸  ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ê²°ê³¼: {'ê¸°ë³¸ì •ë³´': ['ì§€ì—­:ê²½ê¸°', 'ì—°ë ¹ëŒ€:30ëŒ€', 'ì„±ë³„:ì—¬']}\n",
      "\n",
      "[2.5ë‹¨ê³„] ì¹´í…Œê³ ë¦¬ë³„ ë©”íƒ€ë°ì´í„° í•„í„° ì¶”ì¶œ\n",
      "   [ê¸°ë³¸ì •ë³´] í•„í„°: {'ì§€ì—­': 'ê²½ê¸°', 'ì—°ë ¹ëŒ€': '30ëŒ€', 'ì„±ë³„': 'ì—¬'}\n",
      "\n",
      "[3ë‹¨ê³„] ìì—°ì–´ í…ìŠ¤íŠ¸ ìƒì„±\n",
      "\n",
      "ğŸ“ [ê¸°ë³¸ì •ë³´] ìƒì„±: ê²½ê¸° ì§€ì—­ì— ê±°ì£¼í•˜ì‹œëŠ” 30ëŒ€ ì—¬ì„±ì…ë‹ˆë‹¤....\n",
      "\n",
      "[4ë‹¨ê³„] ì„ë² ë”© ìƒì„±\n",
      "âœ… [ê¸°ë³¸ì •ë³´] ì„ë² ë”© ìƒì„± ì™„ë£Œ\n",
      "\n",
      "[5ë‹¨ê³„] ê²€ìƒ‰ ê°€ëŠ¥í•œ íŒ¨ë„: 1000ê°œ\n",
      "\n",
      "ë‹¨ê³„ì  í•„í„°ë§ ì‹œì‘ (ë³‘ë ¬ ê²€ìƒ‰ í™œì„±í™”, workers=5)\n",
      "   ì´ˆê¸° í›„ë³´: 1000ê°œ\n",
      "   ì¹´í…Œê³ ë¦¬ ìˆœì„œ: ['ê¸°ë³¸ì •ë³´']\n",
      "================================================================================\n",
      "\n",
      "[1ë‹¨ê³„] ê¸°ë³¸ì •ë³´ ì¹´í…Œê³ ë¦¬ë¡œ ê²€ìƒ‰ (ë³‘ë ¬)\n",
      "   ë©”íƒ€ë°ì´í„° í•„í„°: {'ì§€ì—­': 'ê²½ê¸°', 'ì—°ë ¹ëŒ€': '30ëŒ€', 'ì„±ë³„': 'ì—¬'}\n",
      "   -> 19ê°œ í›„ë³´ ì„ ë³„\n",
      "\n",
      "ìµœì¢… 10ê°œ íŒ¨ë„ ì„ ë³„ ì™„ë£Œ\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ìµœì¢… ê²€ìƒ‰ ê²°ê³¼\n",
      "================================================================================\n",
      "\n",
      "ì´ 10ê°œ íŒ¨ë„ ë°œê²¬\n",
      "\n",
      "íŒ¨ë„ ëª©ë¡:\n",
      "  1. w301127188993716\n",
      "  2. w443172126365928\n",
      "  3. w4705577419366\n",
      "  4. w164848980649182\n",
      "  5. w43188116147600\n",
      "  6. w243716720104879\n",
      "  7. w163792443900623\n",
      "  8. w207960921756314\n",
      "  9. w179370539731870\n",
      "  10. w248017560406207\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ìƒì„¸ ë©”íƒ€ë°ì´í„° í™•ì¸ì„ ìœ„í•´ ì•„ë˜ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: \"ì„œìš¸ 20ëŒ€ ë‚¨ì\"\n",
    "# â†’ ë©”íƒ€ë°ì´í„° í•„í„°: {\"ì§€ì—­\": \"ì„œìš¸\", \"ì—°ë ¹ëŒ€\": \"20ëŒ€\", \"ì„±ë³„\": \"ë‚¨\"}\n",
    "test_query = \"ê²½ê¸° 30ëŒ€ ì—¬ì\"\n",
    "\n",
    "# ê²€ìƒ‰ ì‹¤í–‰\n",
    "results = pipeline.search(test_query, top_k=10)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ìµœì¢… ê²€ìƒ‰ ê²°ê³¼\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nì´ {len(results)}ê°œ íŒ¨ë„ ë°œê²¬\")\n",
    "\n",
    "if len(results) > 0:\n",
    "    print(\"\\níŒ¨ë„ ëª©ë¡:\")\n",
    "    for i, mb_sn in enumerate(results, 1):\n",
    "        print(f\"  {i}. {mb_sn}\")\n",
    "else:\n",
    "    print(\"\\nì¡°ê±´ì— ë§ëŠ” íŒ¨ë„ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nìƒì„¸ ë©”íƒ€ë°ì´í„° í™•ì¸ì„ ìœ„í•´ ì•„ë˜ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ í™•ì¸\n",
    "\n",
    "**ì£¼ì˜**: ì´ ì…€ì€ ê²€ìƒ‰ ê²°ê³¼(results)ê°€ ìˆì„ ë•Œë§Œ ì‹¤í–‰í•˜ì„¸ìš”."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
